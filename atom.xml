<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blazar</title>
  
  
  <link href="https://jockerlin.github.io/atom.xml" rel="self"/>
  
  <link href="https://jockerlin.github.io/"/>
  <updated>2023-05-27T12:08:09.287Z</updated>
  <id>https://jockerlin.github.io/</id>
  
  <author>
    <name>Blazar</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>人工智障瞎学</title>
    <link href="https://jockerlin.github.io/2023/05/27/machine_learning/"/>
    <id>https://jockerlin.github.io/2023/05/27/machine_learning/</id>
    <published>2023-05-27T10:21:23.742Z</published>
    <updated>2023-05-27T12:08:09.287Z</updated>
    
    <content type="html"><![CDATA[<h1 id="人工智障瞎学"><a href="#人工智障瞎学" class="headerlink" title="人工智障瞎学"></a>人工智障瞎学</h1><h1 id="一、环境配置"><a href="#一、环境配置" class="headerlink" title="一、环境配置"></a>一、环境配置</h1><p>tensorflow-gup&#x3D;&#x3D;2.0</p><p>pytorch&#x3D;&#x3D;1.3.0</p><p>cuda&#x3D;&#x3D;10.1</p><p>cudnn&#x3D;&#x3D;7.6.4</p><p>nvidia-418</p><p><strong>tensorflow2.0 解决pycharm自动补全与应用bug</strong>(reference csy)</p><p>注：<br>修改里我们仅仅修改了keras的import路径，所以keras需要使用from tensorflow import keras;<br>而import keras中其他的包例如layers，需要写成from tensorflow.python.keras import xxxx;</p><p>Go to the dir &#x2F;python3&#x2F;site-packages&#x2F; and change the name of &#x2F;tensorflow&#x2F; to &#x2F;tensorflow_back&#x2F;, then change the name of &#x2F;tensorflow_core&#x2F; to &#x2F;tensorflow&#x2F;</p><p>Go to the file &#x2F;tensorflow&#x2F;<strong>init</strong>.py(which was in &#x2F;tensorflow_core&#x2F;), add following codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .python.keras.api._v2 <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow_estimator.python.estimator.api._v2 <span class="keyword">import</span> estimator</span><br></pre></td></tr></table></figure><h1 id="二、基本概念"><a href="#二、基本概念" class="headerlink" title="二、基本概念"></a>二、基本概念</h1><h2 id="1、监督学习与无监督学习"><a href="#1、监督学习与无监督学习" class="headerlink" title="1、监督学习与无监督学习"></a>1、监督学习与无监督学习</h2><p>机器学习中，可以根据学习任务的不同，分为监督学习(Supervised Learning),无监督学习(Unsupervised Learning)、半监督学习(Semi-Supervised Learning)和强化学习(Reinforcement Learning).</p><p>监督学习：就是人们常说的分类，通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的，也就具有了对未知数据进行分类的能力。监督学习中的数据中是提前做好了分类的信息的，如垃圾邮件检测中，他的训练样本是提前存在分类的信息，也就是对垃圾邮件和非垃圾邮件的标记信息</p><p>监督学习中，他的训练样本中是同时包含有特征和标签信息的，</p><p>监督学习中，比较典型的问题就是像上面说的分类问题(Classfication)和回归问题(Regression)</p><p>它们两者最主要的特点就是分类算法中的标签是<strong>离散</strong>的值，就像上面说的邮件分类问题中的标签为{1, -1},分别表示了垃圾邮件和非垃圾邮件</p><p>而回归算法中的标签值一般是<strong>连续</strong>的值，如预测一个人的年龄，一般要根据身高、性别、体重等标签，这是因为年龄是连续的正整数</p><p>无监督学习：是另一种常用的机器学习算法，它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模，无监督学习的样本是不包含标签信息的，只有一定的特征，所以由于没有标签信息，学习过程中并不知道分类结果是否正确。</p><h2 id="2、线性回归、逻辑回归"><a href="#2、线性回归、逻辑回归" class="headerlink" title="2、线性回归、逻辑回归"></a>2、线性回归、逻辑回归</h2><p>回归问题的条件&#x2F;前提：</p><p>1） 收集的数据</p><p>2） 假设的模型，即一个函数，这个函数里含有未知的参数，通过学习，可以估计出参数。然后利用这个模型去预测&#x2F;分类新的数据。</p><p>最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p><p>数据的熵：K-L散度源于信息论。信息论主要研究如何量化数据中的信息。<img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/auto-orient.png" alt="img"></p><p><strong>熵</strong>的主要作用是告诉我们最优编码信息方案的理论下界（存储空间），以及度量数据的信息量的一种方式。理解了熵，我们就知道有多少信息蕴含在数据之中，现在我们就可以计算当我们用一个带参数的概率分布来近似替代原始数据分布的时候，到底损失了多少信息。</p><p><a href="https://www.jianshu.com/p/43318a3dc715?from=timeline&isappinstalled=0">如何理解K-L散度相对熵</a></p><p>熵的本质是香农信息量的期望。现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布，H(p,q)我们称之为“交叉熵”。</p><p>交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。</p><p>信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。</p><p>根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，记住，信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的【最小努力】（猜题次数、编码长度等）的大小就是信息熵。</p><p><strong>梯度下降</strong>：</p><p>批量梯度下降法(Batch Gradient Descent, BGD)：是梯度下降法的最原始形式，每迭代一步或更新每一参数时，都要用到训练集中的所有样本数据，当样本数目很多时，训练过程会很慢。</p><p>随机梯度下降法(Stochastic Gradient Descent, SGD)：由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法正是为了解决批量梯度下降法这一弊端而提出的。随机梯度下降是通过每个样本来迭代更新一次。SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着最优化方向进行。</p><p>小批量梯度下降法(Mini-Batch Gradient Descent, MBGD)：在每次更新参数时使用m’个样本, m’可能远小于m。</p><h2 id="3、卷积池化全连接-激活、Dropout-BNL"><a href="#3、卷积池化全连接-激活、Dropout-BNL" class="headerlink" title="3、卷积池化全连接,激活、Dropout BNL"></a>3、卷积池化全连接,激活、Dropout BNL</h2><p>CNN由输入层、卷积层、激活函数、池化层、全连接层组成。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>特征提取，滑窗</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/convolve.png" alt="conv"></p><p>激活函数：</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/active_function.jpg" alt="img"></p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>提取局部均值与最大值，有average pooling 与max pooling</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/maxpolling.png" alt="img"></p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>把卷積輸出的二維特徵圖轉換成一維的向量，在整个卷积神经网络中起到“分类器”的作用，把特征representation整合到一起，输出为一个值。全连接层之前的作用是提取特征，全连接层的作用是分类。</p><p>怎么样把3x3x5的输出，转换成1x4096的形式？靠的就是全连接层</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/fc1.jpg" alt="fc1"></p><p>可以理解为在中间做了一个卷积</p><p>用一个3x3x5的filter 去卷积激活函数的输出，得到的结果就是一个fully connected layer 的一个神经元的输出，这个输出就是一个值</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/fc2.jpg" alt="fc2"></p><p>全连接层的操作把分布式特征representation映射到样本标记空间(把特征representation整合到一起，输出为一个值),空间结构特性被忽略了，所以全连接层不适合用于在方位上找Pattern的任务，比如segmentation。</p><p>全连接层之前的作用是提取特征, 全理解层的作用是分类!</p><p><a href="https://zhuanlan.zhihu.com/p/33841176">参考：CNN 入门讲解：什么是全连接层（Fully Connected Layer）?</a></p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>适当地对神经元进行概率为p的丢弃，避免在深层神经网络下较少训练数据的过拟合现象。</p><p>过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/overfitting.jpg" alt="overfitting"></p><p>Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</p><p>Dropout被大量利用于全连接网络，而且一般认为设置为0.5或者0.3，而在卷积网络隐藏层中由于卷积自身的稀疏化以及稀疏化的ReLu函数的大量使用等原因，Dropout策略在卷积网络隐藏层中使用较少。总体而言，Dropout是一个超参，需要根据具体的网络、具体的应用领域进行尝试。</p><p><a href="https://zhuanlan.zhihu.com/p/38200980">深度学习中Dropout原理解析</a></p><h3 id="BatchNormalization-批归一化，标准化"><a href="#BatchNormalization-批归一化，标准化" class="headerlink" title="BatchNormalization(批归一化，标准化)"></a>BatchNormalization(批归一化，标准化)</h3><p>随着网络的深度增加，每层特征值分布会逐渐的向激活函数的输出区间的上下两端（激活函数饱和区间）靠近，这样继续下去就会导致梯度消失。BN就是通过方法将该层特征值分布重新拉回标准正态分布，特征值将落在激活函数对于输入较为敏感的区间，输入的小变化可导致损失函数较大的变化，使得梯度变大，避免梯度消失，同时也可加快收敛。</p><p>数据经过<br>$$<br>\sigma(WX+b)<br>$$</p><p>这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。</p><p>这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。</p><p>通常BN网络层用在卷积层后，用于重新调整数据分布的归一化。</p><p>为什么要归一化？</p><ol><li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li><li>为了程序运行时收敛加快。</li><li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li><li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li><li>保证输出数据中数值小的不被吞食。</li></ol><p>在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p><p>BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。</p><h2 id="4、滑动平均模型"><a href="#4、滑动平均模型" class="headerlink" title="4、滑动平均模型"></a>4、滑动平均模型</h2><p><a href="https://www.e-learn.cn/content/qita/652817">Moving Averages 滑动平均的原理和直观感知</a></p><h2 id="5、感知野"><a href="#5、感知野" class="headerlink" title="5、感知野"></a>5、感知野</h2><p>感受野，用来表示网络内部的不同位置的神经元对原图像的感受范围的大小。</p><p>神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着他可能蕴含更为全局、语义层次更高的特征；而值越小则表示其所包含的特征越趋向于局部和细节。因此感受野的值可以大致用来判断每一层的抽象层次。</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/see_field.jpg" alt="see_field"></p><p>在Conv1中的每一个单元所能看到的原始图像范围是3*3，而由于Conv2的每个单元都是由2×2范围的Conv1构成，因此回溯到原始图像，其实是能够看到5×5的原始图像范围的。因此我们说Conv1的感受野是3，Conv2的感受野是5. 输入图像的每个单元的感受野被定义为1</p><p><a href="https://zhuanlan.zhihu.com/p/28492837">参考：深度神经网络中的感受野(Receptive Field)</a></p><h2 id="6、SVM"><a href="#6、SVM" class="headerlink" title="6、SVM"></a>6、SVM</h2><p>在机器学习中，支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为<strong>非概率二元线性分类器</strong>。</p><p>优化目标为:最大化分类间隔。通俗地来讲，区分两堆点用线区分，区分两堆线要用面，总是要使用高一维度的数据来做分类器的判定原则。</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master//svm.jpg" alt="svm"></p><h3 id="类别"><a href="#类别" class="headerlink" title="类别"></a>类别</h3><ul><li>线性可分SVM</li></ul><p>当训练数据线性可分时，通过硬间隔(hard margin，什么是硬、软间隔下面会讲)最大化可以学习得到一个线性分类器，即硬间隔SVM，如上图的的H3。</p><p>什么是支持向量？<br>在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量(support vector)，也即所有在图中虚线上的点。如下图所示:</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/svm_support_vector.jpg" alt="svm"></p><p>在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用。如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响。也即支持向量对模型起着决定性的作用，这也是“支持向量机”名称的由来。</p><ul><li>线性SVM</li></ul><p>当训练数据不能线性可分但是可以近似线性可分时(不能完全分割，有部分数据错误分类)，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM。</p><ul><li>非线性SVM</li></ul><p>当训练数据线性不可分时，通过使用核技巧(kernel trick， 空间转化)和软间隔最大化，可以学习到一个非线性SVM。</p><p><a href="https://zhuanlan.zhihu.com/p/49331510">看了这篇文章你还不懂SVM你就来打我</a></p><h3 id="总结-SVM"><a href="#总结-SVM" class="headerlink" title="总结(SVM)"></a>总结(SVM)</h3><p>支持向量机的优点是:</p><ul><li>1、由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。</li><li>2、不仅适用于线性线性问题还适用于非线性问题(用核技巧)。</li><li>3、拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。</li><li>4、理论基础比较完善(例如神经网络就更像一个黑盒子)。</li><li>5、支持向量机的缺点是:</li></ul><p>二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)<br>只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)</p><h2 id="7、损失函数计算和优化方法"><a href="#7、损失函数计算和优化方法" class="headerlink" title="7、损失函数计算和优化方法"></a>7、损失函数计算和优化方法</h2><p>各类损失函数</p><table><thead><tr><th>loss 方法</th><th>具体计算</th><th></th></tr></thead><tbody><tr><td>Binary Crossentropy</td><td>使用<strong>交叉熵</strong>损失计算真实标签与预测标签</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><p>各类优化方法</p><h2 id="8、Select-Search"><a href="#8、Select-Search" class="headerlink" title="8、Select Search"></a>8、Select Search</h2><p>基本步骤:<br>基于图的图像分割方法将图像分割成很多小块<br>使用贪心策略，计算每两个相邻区域的相似度<br>合并最相似的两块，直到最终剩下一块完整的图片<br>保存每次产生的图像快包括合并的图像</p><p>HOG</p><p>9、training set、epoch与batch区别</p><p>以CIFAR10数据集举例。CIFAR10数据集合有50000张图片，training set训练集为5w。Epoch为要把训练集轮几遍的意思，若epoch&#x3D;&#x3D;10，则会让网络轮10遍训练集。Batch是针对每个Epoch而言，若设置Batch Size&#x3D;&#x3D;100，则每个epoch中有(5w&#x2F;100&#x3D;)500个batch，会更新500次权重。</p><p>归纳：5w张训练图像，先100张图像一份，分成500份，每训练1份(100张)图像，更新一次参数，直至训练了500次，完成了一个epoch的训练。</p><h2 id="n-1、"><a href="#n-1、" class="headerlink" title="n-1、"></a>n-1、</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="literal">None</span>, <span class="number">784</span>])</span><br></pre></td></tr></table></figure><p>x不是一个特定的值，而是一个占位符<code>placeholder</code>，我们在TensorFlow运行计算时输入这个值。我们希望能够输入任意数量的MNIST图像，每一张图展平成784维的向量。我们用2维的浮点数张量来表示这些图，这个张量的形状是<code>[None，784 ]</code>。（这里的<code>None</code>表示此张量的第一个维度可以是任何长度的。）</p><p>需要学习的参数值使用tf.variable</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><p>第四章摘要 神经网络工具箱<br>我们赋予<code>tf.Variable</code>不同的初值来创建不同的<code>Variable</code>：在这里，我们都用全为零的张量来初始化<code>W</code>和<code>b</code>。因为我们要学习<code>W</code>和<code>b</code>的值，它们的初值可以随意设置。</p><p>nn中还有一个很常用的模块：<code>nn.functional</code>，nn中的大多数layer，在<code>functional</code>中都有一个与之相对应的函数。<code>nn.functional</code>中的函数和<code>nn.Module</code>的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由<code>class layer(nn.Module)</code>定义，会自动提取可学习的参数。而<code>nn.functional</code>中的函数更像是纯函数，由<code>def function(input)</code>定义。</p><p>如果模型有可学习的参数，最好用nn.Module，如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module。<br>其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。</p><p>ResNet</p><p>特征描述子应对目标识别，老旧的特征识别算子 SIFT HOG</p><p>方向梯度直方图（Histogram of Oriented Gradient, HOG）HOG+SVM进行行人检测</p><p><a href="https://blog.csdn.net/zouxy09/article/details/7929348">目标检测的图像特征提取之（一）HOG特征</a></p><p>HOG特征提取：</p><p>1）灰度化（将图像看做一个x,y,z（灰度）的三维图像）；</p><p>2）采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）；目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰；</p><p>3）计算图像每个像素的梯度（包括大小和方向）；主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。</p><p>4）将图像划分成小cells（例如6*6像素&#x2F;cell）；</p><p>5）统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor；</p><p>6）将每几个cell组成一个block（例如3*3个cell&#x2F;block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。</p><p>7）将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor了。这个就是最终的可供分类使用的特征向量了。</p><p>fine-tune<br>Fine-tune时可以选择fine-tune全部层或部分层。通常，前面的层提取的是图像的通用特征（generic features）（例如边缘检测，色彩检测），这些特征对许多任务都有用。后面的层提取的是与特定类别有关的特征，因此fine-tune时常常只需要Fine-tuning后面的层。</p><h2 id="n、计算图"><a href="#n、计算图" class="headerlink" title="n、计算图"></a>n、计算图</h2><p>在PyTorch中计算图的特点可总结如下：</p><ul><li><p>autograd根据用户对variable的操作构建其计算图。对变量的操作抽象为<code>Function</code>。</p></li><li><p>对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的<code>grad_fn</code>为None。叶子节点中需要求导的variable，具有<code>AccumulateGrad</code>标识，因其梯度是累加的。</p></li><li><p>variable默认是不需要求导的，即<code>requires_grad</code>属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点<code>requires_grad</code>都为True。</p></li><li><p>variable的<code>volatile</code>属性默认为False，如果某一个variable的<code>volatile</code>属性被设为True，那么所有依赖它的节点<code>volatile</code>属性都为True。volatile属性为True的节点不会求导，volatile的优先级比<code>requires_grad</code>高。</p></li><li><p>多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定<code>retain_graph</code>&#x3D;True来保存这些缓存。</p></li><li><p>非叶子节点的梯度计算完之后即被清空，可以使用<code>autograd.grad</code>或<code>hook</code>技术获取非叶子节点的值。</p></li><li><p>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播</p></li><li><p>反向传播函数<code>backward</code>的参数<code>grad_variables</code>可以看成链式求导的中间结果，如果是标量，可以省略，默认为1</p></li><li><p>PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</p></li></ul><p><a href="https://github.com/chenyuntc/pytorch-book">pytorch-book</a></p><h1 id="三、常见的模型结构"><a href="#三、常见的模型结构" class="headerlink" title="三、常见的模型结构"></a>三、常见的模型结构</h1><p><a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD</a></p><h2 id="1、AlexNet"><a href="#1、AlexNet" class="headerlink" title="1、AlexNet"></a>1、AlexNet</h2><p>AlexNet的网络结构图：</p><p>原始的网络采用上下两个GPU并行运算，在特定的二、四、五层进行数据交互。</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/AlexNet_detail.png" alt="AlexNet_detail"></p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/AlexNet_struct.jpg" alt="AlexNet_struct"></p><p>各层之间的数据传递:</p><table><thead><tr><th>input vector size</th><th>layer</th><th>cal</th></tr></thead><tbody><tr><td>227×227×3</td><td><strong>conv1</strong>: 11×11×3, stride:4, kernels:96</td><td>(227-11)&#x2F;4+1&#x3D;55</td></tr><tr><td>55×55×96</td><td><strong>pool</strong>: 3×3,  stride:2</td><td>(55-3)&#x2F;2+1&#x3D;27</td></tr><tr><td>27×27×96</td><td><strong>conv2</strong>: 5×5×96, padding:4, kernels:256</td><td>(27-5+2×2)&#x2F;2+1&#x3D;27</td></tr><tr><td>27×27×256</td><td><strong>pool</strong>: 3×3,  stride:2</td><td>(27-3)&#x2F;2+1&#x3D;13</td></tr><tr><td>13×13×256</td><td><strong>conv3</strong>: 3×3×256, padding:1, kernels:384</td><td>(13-3+2×1)&#x2F;2+1&#x3D;13</td></tr><tr><td>13×13×384</td><td><strong>conv4</strong>: 3×3×384, padding:1, kernels:384</td><td>(13-3+2×1)&#x2F;2+1&#x3D;13</td></tr><tr><td>13×13×384</td><td><strong>conv5</strong>: 3×3×384, padding:1, kernels:256</td><td>(13-3+2×1)&#x2F;2+1&#x3D;13</td></tr><tr><td>13×13×256</td><td><strong>pool</strong>: 3×3,  stride:2</td><td>(13-3)&#x2F;2+1&#x3D;6</td></tr><tr><td>6×6×256</td><td><strong>FC1</strong>: 6×6×256×4096</td><td>pass</td></tr><tr><td>1×4096</td><td><strong>FC2</strong>: 4096×4096</td><td>pass</td></tr><tr><td>1×4096</td><td><strong>FC2</strong>: 4096×1000</td><td>finally output: 1×1000</td></tr></tbody></table><p>AlexNet网络结构共有8层，前面5层是卷积层，后面3层是全连接层，最后一个全连接层的输出传递给一个1000路的softmax层，对应1000个类标签的分布。</p><p>由于AlexNet采用了两个GPU进行训练，因此，该网络结构图由上下两部分组成，一个GPU运行图上方的层，另一个运行图下方的层，两个GPU只在特定的层通信。例如第二、四、五层卷积层的核只和同一个GPU上的前一层的核特征图相连，第三层卷积层和第二层所有的核特征图相连接，全连接层中的神经元和前一层中的所有神经元相连接。</p><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/31717727">CNN模型之AlexNet</a></p><p><a href="https://blog.csdn.net/zyqdragon/article/details/72353420">深度学习AlexNet模型详细分析</a></p><p><a href="https://zhuanlan.zhihu.com/p/33841176">CNN 入门讲解：什么是全连接层（Fully Connected Layer）?</a></p><h2 id="2、VGG"><a href="#2、VGG" class="headerlink" title="2、VGG"></a>2、VGG</h2><p>VGG是Oxford的Visual Geometry Group的组提出的，主要有两种结构，分别是VGG16和VGG19，两者并没有本质上的区别，只是网络深度不一样。</p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><a href="https://dgschwend.github.io/netscope/#/preset/vgg-16">vgg高清无码结构</a></p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/vgg_net.jpg" alt="vgg_net"></p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。</p><p>比如，3个步长为1的3x3卷积核的一层层叠加作用可看成一个大小为7的<strong>感受野</strong>（其实就表示3个3x3连续卷积相当于一个7x7卷积），其参数总量为 3x(9xC^2) ，如果直接使用7x7卷积核，其参数总量为 49xC^2 ，这里 C 指的是输入和输出的通道数。很明显，27xC^2小于49xC^2，即减少了参数；而且3x3卷积核有利于更好地保持图像性质。</p><p>为什么使用2个3x3卷积核可以来代替5*5卷积核：</p><p>5x5卷积看做一个小的全连接网络在5x5区域滑动，我们可以先用一个3x3的卷积滤波器卷积，然后再用一个全连接层连接这个3x3卷积输出，这个全连接层我们也可以看做一个3x3卷积层。这样我们就可以用两个3x3卷积级联（叠加）起来代替一个5x5卷积。</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/replace_convolutions.jpg" alt="replace_convolutions"></p><h3 id="VGG优缺点"><a href="#VGG优缺点" class="headerlink" title="VGG优缺点"></a>VGG优缺点</h3><p>优点：</p><p>VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。<br>几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：<br>验证了通过不断加深网络结构可以提升性能。</p><p>缺点：</p><p>VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊！</p><h2 id="3、R-CNN"><a href="#3、R-CNN" class="headerlink" title="3、R-CNN"></a>3、R-CNN</h2><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，<strong>去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练</strong>，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样<strong>相比于你直接采用随机初始化的方法，精度可以有很大的提高</strong>。</p><p>对于目标检测问题： 图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，<strong>如何用少量的标注数据，训练高质量的模型</strong>，这就是文献最大的特点，这篇论文采用了迁移学习的思想： 先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片分类训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段CNN模型的输出是1000个神经元（当然也<strong>直接可以采用Alexnet</strong>训练好的模型参数）。</p><p>基于Alexnet，设计fine-tuning CNN模型，softmax分类输出层为21个神经元。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>采用迁移学习，提取 ILSVRC(ImageNet Large Scale Visual Recognition Competition)2012 的模型与权重，在VOC(Visual Object Classes)上进行fine-tune(微调)。<br>当前训练学习的是识别类型的能力，不是预测bbox的能力，</p><p>R-CNN 将候选区域与 GroundTrue 中的 box 标签相比较，如果 IoU &gt; 0.5，说明两个对象重叠的位置比较多，于是就可以认为这个候选区域是 Positive，否则就是 Negetive。</p><p>训练策略是：采用 SGD 训练，初始学习率为 0.001，mini-batch 大小为 128。</p><p>R-CNN的三个步骤<br>1、候选区域生成：给定一张输入图片，从图片中提取 2000 个类别独立的候选区域（采用Selective Search 方法）。<br>2、特征提取：对于每个区域利用 CNN 抽取一个固定长度的特征向量。<br>3、类别判断：再对每个区域利用 SVM 进行目标分类。<br>4、位置精修：使用回归器精细修正候选框位置bbox</p><ul><li>候选区域生成</li></ul><p>输入:image，输出:200个227*227的bbox</p><p>生产类别独立的候选区域，这些候选区域其中包含了 R-CNN 最终定位的结果，使用selective search算法，生成候选区域。<br>select search 使用felzenszwalb segmentation 生成。<br>搜出的候选框是矩形的，而且是大小各不相同，需要缩放处理成相同尺寸丢进CNN(输入227*227)。经过最后的试验，论文作者发现先padding&#x3D;16扩充后，然后再采用“各向异性缩放”到固定大小精度最高。</p><ul><li>CNN特征提取</li></ul><p>输入:image中的bbox image，输出:固定长度的特征向量</p><p>a、网络结构设计<br>选用<a href="#alexnet">Alexnet</a>，Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/AlexNet.jpg" alt="AlexNet.jpg"></p><p>b、网络有监督预训练分类阶段（图片数据库：ImageNet ILSVC)<br>参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这里文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解时采用随机梯度下降法，学习率大小为0.001；</p><p>C、fine-tuning检测阶段（图片数据库：PASCAL VOC）<br>我们接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg &#x3D; 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个正样本、96个负样本。</p><p>正负样本定义：<br>一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。</p><p>CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了。</p><p><a href="https://blog.csdn.net/Eddy_zheng/article/details/52126641">检测评价函数iou</a></p><ul><li>SVM分类器</li></ul><p>输入:CNN产生的特征向量，输出:类别及其分数？</p><p>将每个提议区域的特征连同其标注的类别作为⼀个样本，训练多个⽀持向量机对⽬标分类。其中每个⽀持向量机⽤来判断样本是否属于某⼀个类别。</p><p>一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000<em>4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096</em>N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)（对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数）就可以得到结果了。</p><ul><li>位置精修</li></ul><p>将每个提议区域的特征连同其标注的边界框作为⼀个样本，训练线性回归模型来预测真实边界框。</p><!-- 得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background)。 排序，canny边界检测之后就得到了我们需要的bounding-box。 --><p>测试阶段的目标检测</p><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>思路是挑选一个特征出来，把它直接当成一个物体分类器，然后计算它们处理不同的候选区域时，activation的值，这个值代表了特征对这块区域的响应情况，然后将activation作为分数排名，取前几位，然后显示这些候选区域，自然也可以清楚明白，这个feature大概是什么。</p><p>R-CNN 作者将 pool5 作为可视化对象，它的 feature map 是 6x6x255 的规格，可以理解为有 256 个小方块，每个方块对应一个特征。</p><h3 id="R-CNN总结"><a href="#R-CNN总结" class="headerlink" title="R-CNN总结"></a>R-CNN总结</h3><p>采用 AlexNet<br>采用 Selective Search 技术生成 Region Proposal<br>在 ImageNet 上先进行预训练，然后利用成熟的权重参数在 PASCAL VOC 数据集上进行 fine-tune<br>用 CNN 抽取特征，然后用一系列的的 SVM 做类别预测。<br>的 bbox 位置回归基于 DPM 的灵感，自己训练了一个线性回归模型。<br>的语义分割采用 CPMC 生成 Region</p><p>此paper采用的方法是：首先输入一张图片，我们先定位出2000个物体候选框，然后采用CNN提取每个候选框中图片的特征向量，特征向量的维度为4096维，接着采用svm算法对各个候选框中的物体进行分类识别</p><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><p>R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。<br>为了提速，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。</p><p>但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。</p><p>SPP：Spatial Pyramid Pooling（空间金字塔池化）<br>SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？</p><p>简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。</p><h3 id="SPP-空间金字塔池化"><a href="#SPP-空间金字塔池化" class="headerlink" title="SPP 空间金字塔池化"></a>SPP 空间金字塔池化</h3><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h2 id="4、fast-R-CNN"><a href="#4、fast-R-CNN" class="headerlink" title="4、fast R-CNN"></a>4、fast R-CNN</h2><p>Fast R-CNN就是在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。</p><p>tips：<a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/2139">边框回归</a></p><p>之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression，而在Fast R-CNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。</p><p>R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。<br>大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。<br>解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征</p><p>原来的方法：许多候选框（比如两千个）–&gt;CNN–&gt;得到每个候选框的特征–&gt;分类+回归<br>现在的方法：一张完整图片–&gt;CNN–&gt;得到每张候选框的特征–&gt;分类+回归</p><p>所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。</p><h2 id="5、faster-R-CNN"><a href="#5、faster-R-CNN" class="headerlink" title="5、faster R-CNN"></a>5、faster R-CNN</h2><p>参考与引用：</p><p><a href="https://zhuanlan.zhihu.com/p/31426458">文章：一文读懂Faster RCNN</a></p><p><a href="https://github.com/jwyang/faster-rcnn.pytorch">code:faster-rcnn.pytorch</a></p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/faster_RCNN_net.jpg" alt="faster_rcnn"></p><p>相比较于fast rcnn,加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。一个神经网络层所提取到的feature既用于bbox的选择，也用于最后的object detection。</p><h3 id="PRN-region-proposal-network"><a href="#PRN-region-proposal-network" class="headerlink" title="PRN(region proposal network)"></a>PRN(region proposal network)</h3><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/RPN_flow.jpg" alt="rpn_flow"></p><p>上图(rpn_flow)展示了RPN网络的具体结构。</p><p>可以看到RPN网络实际分为<strong>2条主线</strong></p><p>上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。</p><p>而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p><p>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</p><ul><li>anchors</li></ul><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/anchors.jpg" alt="anchors"></p><p>每个feature map的最小单位对应一个anchors，固定面积根据长宽比例生成9个对应的候选框。</p><ul><li>bounding box regression</li></ul><p>寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’,用线性回归来建模对窗口进行微调。对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度，显然即可用来修正Anchor位置了。</p><p>回到图(<a href="faster_rcnn">faster_rcnn</a>)，VGG输出 50<em>38</em>512 的特征，对应设置 50<em>38</em>k 个anchors，而RPN输出：</p><p>1.大小为50<em>38</em>2k 的positive&#x2F;negative softmax分类特征矩阵</p><p>2.大小为50<em>38</em>4k 的regression坐标回归特征矩阵<br>恰好满足RPN完成positive&#x2F;negative分类+bounding box regression坐标回归.</p><ul><li>proposal layer</li></ul><p>Proposal Layer负责综合所有 [dx(A), dy(A), dw(A), dh(A)] 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。</p><p>Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出</p><p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p><ol><li><p>生成anchors，利用[dx(A), dy(A), dw(A), dh(A)]对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</p></li><li><p>按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors</p></li><li><p>限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界</p></li><li><p>剔除尺寸非常小的positive anchors</p></li><li><p>对剩余的positive anchors进行NMS（nonmaximum suppression）</p></li></ol><p>之后输出proposal&#x3D;[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。严格意义上的检测应该到此就结束了，后续部分应该属于识别了。</p><p>RPN网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</strong></p><h3 id="POI-Pulling"><a href="#POI-Pulling" class="headerlink" title="POI Pulling"></a>POI Pulling</h3><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/roi_polling.jpg" alt="roi_polling"></p><p>该层的两个输入：</p><ol><li>原始的feature maps</li><li>RPN输出的proposal boxes（大小各不相同）</li></ol><p>处理后，即使大小不同的proposal输出结果都是 pooled_w*polled_h 固定大小，实现了固定长度输出。</p><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/fasterRCNN_classification.jpg" alt="roi_polling"></p><p>Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框</p><p>Fast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？</p><h2 id="6、mask-R-CNN"><a href="#6、mask-R-CNN" class="headerlink" title="6、mask R-CNN"></a>6、mask R-CNN</h2><h2 id="7、YOLOv3"><a href="#7、YOLOv3" class="headerlink" title="7、YOLOv3"></a>7、YOLOv3</h2><p>key words:端到端，darknet-53，参差网络、BN+LeakyRelu</p><p>yolov3没有使用polling和FC，v2中的池化方法在v3中使用增大步长的卷积核实现，在backbone中，会有5次增大步长的卷积操作这也是从输入416x416&#x3D;&gt;13*13的原因(下降了2^5)。</p><p>所使用的特征层是32的倍数</p><p>借鉴FPN(feature pyramid networks)，输出了三个feature map，多尺度进行目标检测，三个feature map的深度都是255，边长比52:26:13。每次对应的感受野不同，32倍降采样的感受野最大。</p><p>当输入为416x416时，实际总共有（52x52+26x26+13x13）x3&#x3D;10647个proposal box。</p><p>对于coco数据集合，80个类别，每个网格预测三个box，每个box有(x,y,w,h,confidence)五个参数,再加上80个类别的概率，所以每个feature map的深度&#x3D;3*(5+80)&#x3D;255。</p><p>张量拼接，与后面网络层的上采样结果进行一个拼接后的处理结果作为feature map的输出。</p><p>details：</p><p>bbox的预测：</p><p>直接运用逻辑回归预测bbox中心点相对于网络单元左上角的相对位置。给定9个在数据集上聚类的参数，分别是宽和高。设置阈值对模板框进行筛选。<br>V3只会在一个模板框上面进行预测，(如何在9个中选择1个？？？逻辑回归实现，用曲线对模板框相对于目标物体的分数映射关系)，每个feature map使用9&#x2F;&#x2F;3个模板框。</p><p>loss function:</p><p>x,y,w,h使用the mean squared error (squared L2 norm)计算<br>class,confidence使用Binary Cross Entropy二值交叉熵<br>总loss将以上相加得到。</p><p>训练分析：</p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/computer_device.png" alt="computer device"></p><p><img src="https://gitee.com/BlazarLin/notes-image-library/raw/master/yolov3_loss.png" alt="yolov3 loss"></p><p>yolov3 网络结构比较大，且输入为416x416，由于显存的限制,训练的时候batch size只能为1。</p><p>从上图可以看出，在前500个batch，loss下降的比较明显，而从200～10k batch的时候，l在所以几乎oss几乎没什么变化，一直在10-20徘徊。</p><p>实验在一个epoch后就不怎么收敛了，需要显存够大的情况下训练效果才比较明显。(to do)</p><p>yolov3-tiny batch size能够设置到16。</p><h2 id="训练日志"><a href="#训练日志" class="headerlink" title="训练日志"></a>训练日志</h2><p>yolo-tiny在epoch5-50的迭代中，训练loss一直是3~4左右。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后总结一下各大算法的步骤：<br>RCNN<br>1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)<br>2.每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取<br>3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类<br>4.对于属于某一类别的候选框，用回归器进一步调整其位置</p><p>Fast R-CNN<br>1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)<br>2.对整张图片输进CNN，得到feature map<br>3.找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层<br>4.对候选框中提取出的特征，使用分类器判别是否属于一个特定类<br>5.对于属于某一类别的候选框，用回归器进一步调整其位置</p><p>Faster R-CNN<br>1.对整张图片输进CNN，得到feature map<br>2.卷积特征输入到RPN，得到候选框的特征信息<br>3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类<br>4.对于属于某一类别的候选框，用回归器进一步调整其位置</p><p>R-CNN（Selective Search + CNN + SVM）<br>SPP-net（ROI Pooling）<br>Fast R-CNN（Selective Search + CNN + ROI）<br>Faster R-CNN（RPN + CNN + ROI Pooling）</p><p>Mask RCNN<br>1.</p><p>YOLOV3<br>1.</p><p>参考与引用</p><p><a href="https://zhuanlan.zhihu.com/p/49331510">看了这篇文章你还不懂SVM你就来打我</a></p><p><a href="https://zhuanlan.zhihu.com/p/23006190">RCNN- 将CNN引入目标检测的开山之作</a></p><p><a href="https://blog.csdn.net/Eddy_zheng/article/details/52126641">检测评价函数 intersection-over-union （ IOU ）</a></p><h1 id="四、数据集"><a href="#四、数据集" class="headerlink" title="四、数据集"></a>四、数据集</h1><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/">The PASCAL Visual Object Classes Homepage</a></p><h1 id="五、疑问"><a href="#五、疑问" class="headerlink" title="五、疑问"></a>五、疑问</h1><ul><li>1、TensorFlow GPU:ran out of memory trying to allocate 1.14GiB</li></ul><p>遇到类似问题，应该减小BatchSize。cifar10数据集train image size为[32,32]，在将模型切换到官方VGG16版本的时候，需要将第一层数据缩放至[224,224]，所以，先前的batch size就太大了，gpu一次吃不了一个batch的数据，导致警告，甚至无法训练，就需要减小batch size的尺寸。</p><h1 id="六、所有参考来源"><a href="#六、所有参考来源" class="headerlink" title="六、所有参考来源"></a>六、所有参考来源</h1><p><a href="https://www.jianshu.com/p/43318a3dc715?from=timeline&isappinstalled=0">如何理解K-L散度（相对熵）</a></p><p><a href="https://zhuanlan.zhihu.com/p/38200980">深度学习中Dropout原理解析</a></p><p><a href="https://www.e-learn.cn/content/qita/652817">Moving Averages 滑动平均的原理和直观感知</a></p><p><a href="https://zhuanlan.zhihu.com/p/28492837">参考：深度神经网络中的感受野(Receptive Field)</a></p><p><a href="https://zhuanlan.zhihu.com/p/49331510">看了这篇文章你还不懂SVM你就来打我</a></p><p><a href="https://blog.csdn.net/zouxy09/article/details/7929348">目标检测的图像特征提取之（一）HOG特征</a></p><p><a href="https://github.com/chenyuntc/pytorch-book">pytorch-book</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/80170182">一文读懂目标检测：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD</a></p><p><a href="https://zhuanlan.zhihu.com/p/31717727">CNN模型之AlexNet</a></p><p><a href="https://blog.csdn.net/zyqdragon/article/details/72353420">深度学习AlexNet模型详细分析</a></p><p><a href="https://zhuanlan.zhihu.com/p/33841176">CNN 入门讲解：什么是全连接层（Fully Connected Layer）?</a></p><p><a href="https://dgschwend.github.io/netscope/#/preset/vgg-16">vgg高清无码结构</a></p><p><a href="https://blog.csdn.net/Eddy_zheng/article/details/52126641">检测评价函数iou</a></p><p><a href="https://www.julyedu.com/question/big/kp_id/26/ques_id/2139">边框回归</a></p><p><a href="https://zhuanlan.zhihu.com/p/31426458">文章：一文读懂Faster RCNN</a></p><p><a href="https://github.com/jwyang/faster-rcnn.pytorch">code:faster-rcnn.pytorch</a></p><p><a href="https://zhuanlan.zhihu.com/p/49331510">看了这篇文章你还不懂SVM你就来打我</a></p><p><a href="https://zhuanlan.zhihu.com/p/23006190">RCNN- 将CNN引入目标检测的开山之作</a></p><p><a href="https://blog.csdn.net/Eddy_zheng/article/details/52126641">检测评价函数 intersection-over-union （ IOU ）</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;人工智障瞎学&quot;&gt;&lt;a href=&quot;#人工智障瞎学&quot; class=&quot;headerlink&quot; title=&quot;人工智障瞎学&quot;&gt;&lt;/a&gt;人工智障瞎学&lt;/h1&gt;&lt;h1 id=&quot;一、环境配置&quot;&gt;&lt;a href=&quot;#一、环境配置&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jockerlin.github.io/2023/05/27/imgprocess/"/>
    <id>https://jockerlin.github.io/2023/05/27/imgprocess/</id>
    <published>2023-05-27T10:21:23.731Z</published>
    <updated>2023-05-27T12:07:52.999Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Image-Process"><a href="#Image-Process" class="headerlink" title="Image Process"></a>Image Process</h1><!-- TOC --><ul><li><a href="#%E5%88%86%E5%89%B2%E8%AF%86%E5%88%AB%E7%BC%BA%E9%99%B7%E7%AD%89%E5%87%A0%E4%B8%AA%E5%A4%A7%E6%96%B9%E5%90%91">分割、识别、缺陷等几个大方向</a><ul><li><a href="#%E9%98%88%E5%80%BC%E6%B3%95">阈值法</a></li><li><a href="#otsu">OTSU</a></li><li><a href="#%E5%8D%B7%E7%A7%AF%E7%AE%97%E5%AD%90%E9%AB%98%E6%96%AFsobel">卷积算子(高斯、sobel)</a></li><li><a href="#%E5%8C%BA%E5%9F%9F%E7%94%9F%E9%95%BF">区域生长</a></li><li><a href="#%E5%88%86%E6%B0%B4%E5%B2%AD">分水岭</a></li><li><a href="#hog--dpm">HOG &amp; DPM</a><ul><li><a href="#hog">HOG</a><ul><li><a href="#%E5%85%B7%E4%BD%93%E5%88%86%E6%9E%90">具体分析</a></li></ul></li><li><a href="#dpmdeformable-part-based-model">DPM(Deformable part-based model)</a></li></ul></li><li><a href="#%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B">关键点检测</a></li><li><a href="#siftscale-invariant-feature-transform-%E5%B0%BA%E5%BA%A6%E4%B8%8D%E5%8F%98%E7%89%B9%E5%BE%81%E8%BD%AC%E6%8D%A2">SIFT(Scale-invariant feature transform 尺度不变特征转换)</a><ul><li><a href="#orb">ORB</a></li></ul></li><li><a href="#hough%E6%A3%80%E6%B5%8B">HOUGH检测</a><ul><li><a href="#%E7%9B%B4%E7%BA%BF">直线</a><ul><li><a href="#%E9%9C%8D%E5%A4%AB%E7%A9%BA%E9%97%B4">霍夫空间</a></li><li><a href="#%E6%A0%87%E5%87%86%E9%9C%8D%E5%A4%AB%E7%BA%BF%E5%8F%98%E6%8D%A2%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B">标准霍夫线变换算法流程</a></li><li><a href="#%E7%BB%9F%E8%AE%A1%E6%A6%82%E7%8E%87%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B">统计概率霍夫变换算法流程</a></li></ul></li><li><a href="#%E5%9C%86">圆</a><ul><li><a href="#%E7%BB%8F%E5%85%B8hough%E5%9C%86">经典hough圆</a></li><li><a href="#hough%E6%A2%AF%E5%BA%A6%E6%B3%95">hough梯度法</a></li></ul></li></ul></li><li><a href="#%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B">角点检测</a></li><li><a href="#%E6%8F%92%E5%80%BC%E6%96%B9%E6%B3%95">插值方法</a></li><li><a href="#%E7%9B%B4%E6%96%B9%E5%9B%BE">直方图</a></li><li><a href="#%E5%88%A9%E7%94%A8%E7%9B%B4%E6%96%B9%E5%9B%BE%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB">利用直方图，进行图像识别</a><ul><li><a href="#knn">KNN</a></li><li><a href="#k-%E5%B9%B3%E5%9D%87%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95k--means-clustering">k-平均聚类算法（k -means Clustering）</a></li></ul></li><li><a href="#linemod-%E6%A8%A1%E6%9D%BF%E5%8C%B9%E9%85%8D">linemod 模板匹配</a></li><li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%8E%9F%E7%90%86">神经网络的搭建与原理</a><ul><li><a href="#%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D">基本介绍</a></li><li><a href="#%E5%88%A9%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E5%9B%BE%E5%83%8Fhog%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E5%AD%A6%E4%B9%A0%E5%88%86%E8%BE%A8%E8%9D%BE%E8%9E%88%E7%9A%84%E5%A4%B4">利用神经网络对图像HOG特征进行学习,分辨蝾螈的头</a><ul><li><a href="#%E6%B6%89%E5%8F%8A%E8%A6%81%E7%82%B9">涉及要点</a></li></ul></li></ul></li><li><a href="#svm">SVM</a></li><li><a href="#%E6%A0%87%E5%AE%9A%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%8E%A5%E5%8F%A3">标定程序设计与接口</a></li><li><a href="#%E5%8F%8C%E7%9B%AE%E6%88%90%E5%83%8F%E7%B3%BB%E7%BB%9F">双目成像系统</a><ul><li><a href="#%E8%A1%8C%E5%AF%B9%E5%87%86%E5%9B%BE%E5%83%8F">行对准图像</a></li><li><a href="#%E7%94%9F%E6%88%90%E6%B7%B1%E5%BA%A6%E5%9B%BE">生成深度图</a></li></ul></li></ul></li></ul><!-- /TOC --><p><a href="https://github.com/opencv/opencv">github:opencv</a><br><a href="https://github.com/opencv/opencv_contrib">github:opencv_contrib</a><br><a href="https://github.com/JockerLin/CV">个人实验代码</a></p><h2 id="分割、识别、缺陷等几个大方向"><a href="#分割、识别、缺陷等几个大方向" class="headerlink" title="分割、识别、缺陷等几个大方向"></a>分割、识别、缺陷等几个大方向</h2><h3 id="阈值法"><a href="#阈值法" class="headerlink" title="阈值法"></a>阈值法</h3><p>有固定阈值与自适应阈值，固定阈值即给定一个像素分界线，像素值大于分界的为positive，像素值小的为negative；自适应阈值在当前像素的block size(n*n(奇数))内选择当前block的局部阈值，将中心像素与局部阈值作比较。</p><p>太粗暴简单，哪怕是自适应的局部阈值法，一样难逃无法分割类内方差较大的目标的宿命。它完全没有利用好像素的空间信息，导致分割结果极其容易受噪声干扰，经常出现断裂的边缘，需要后处理。</p><p>常用接口:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv2.threshold</span><br><span class="line">cv2.adaptiveThreshold</span><br></pre></td></tr></table></figure><p>闭运算：先腐蚀再膨胀，解决轮廓某部分突变</p><p>开运算：先膨胀后腐蚀，消除裂开的轮廓</p><h3 id="图像预处理-形态学"><a href="#图像预处理-形态学" class="headerlink" title="图像预处理-形态学"></a>图像预处理-形态学</h3><h3 id="图像预处理-滤波"><a href="#图像预处理-滤波" class="headerlink" title="图像预处理-滤波"></a>图像预处理-滤波</h3><p>对以下滤波方法分析：均值mean，中值median，高斯gauss，双边bilateral、二项binomial</p><p>所使用的噪声类型：白噪声、椒盐噪声、均值噪声、高斯噪声</p><p>(椒盐噪声：一定信噪比下的随机像素值，“椒”代表胡椒，像素值为0黑色，“盐”代表盐巴，像素值为255白色)</p><h4 id="1-均值mean"><a href="#1-均值mean" class="headerlink" title="1) 均值mean"></a>1) 均值mean</h4><p>对mask区域做像素均值提取<br>$$<br>\texttt{K} &#x3D; \frac{1}{\texttt{ksize.width*ksize.height}} \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1 \ \cdots\cdots \ 1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1 \ \end{bmatrix}<br>$$<br>code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> ksize = <span class="number">5</span>;</span><br><span class="line"><span class="comment">// halcon</span></span><br><span class="line"><span class="built_in">meanImage</span>(TiledImage, &amp;ImageMean, ksize, ksize);</span><br><span class="line"><span class="comment">// cv</span></span><br><span class="line"><span class="built_in">blur</span>(*temp_image, *temp_output_image, <span class="built_in">Size</span>(ksize, ksize));</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-中值median"><a href="#2-中值median" class="headerlink" title="2) 中值median"></a>2) 中值median</h4><p>取mask区域内的中值<br>$$<br>\texttt{value} &#x3D;\frac{\texttt{sum(mask)}}{\texttt{number}}<br>$$<br>code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> ksize = <span class="number">5</span>;</span><br><span class="line"><span class="comment">// halcon</span></span><br><span class="line"><span class="built_in">medianImage</span>(TiledImage, &amp;ImageMedian, <span class="string">&quot;square&quot;</span>, ksize, <span class="string">&quot;mirrored&quot;</span>);</span><br><span class="line"><span class="comment">// cv</span></span><br><span class="line"><span class="built_in">medianBlur</span>(*temp_image, *temp_output_image, ksize);</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="3-高斯gauss"><a href="#3-高斯gauss" class="headerlink" title="3) 高斯gauss"></a>3) 高斯gauss</h4><p>利用正态分布对图像进行模糊化处理</p><p>一维度正态分布：</p><img src="material/gauss_function_1d.png" style="zoom:50%;" /><p>μ为均值，σ为方差，计算的时候，μ是x的均值，σ是x的方差。因为计算平均值的时候，中心点就是原点，所以μ等于0。</p><img src="material/gauss_function_1d_s.png" style="zoom:50%;" /><p>二维正态分布：</p><img src="material/gauss_function_2d.png" style="zoom:50%;" /><img src="material/gauss_filter.png" style="zoom:50%;" /><p>自定义算完核内元素之后，核内中心坐标为<code>(0,0)</code>记得归一化，使<code>sum(kernel)==1</code></p><p>通过指定方差与核宽来生成gauss滤波器，后对图像进行卷积操作，得到轮完之后的新数组(图像)。</p><p>以方差σ&#x3D;1.5，核宽size&#x3D;3为例，滤波器为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">归一化之前：</span><br><span class="line">[[0.04535423 0.05664058 0.04535423]</span><br><span class="line"> [0.05664058 0.07073553 0.05664058]</span><br><span class="line"> [0.04535423 0.05664058 0.04535423]]</span><br><span class="line"></span><br><span class="line">kernel = kernel/sum(kernel)</span><br><span class="line"></span><br><span class="line">归一化之后：</span><br><span class="line">[[0.09474166 0.11831801 0.09474166]</span><br><span class="line"> [0.11831801 0.14776132 0.11831801]</span><br><span class="line"> [0.09474166 0.11831801 0.09474166]]</span><br></pre></td></tr></table></figure><p>visionpro 中高斯采样器的Sigma计算：设平滑值(高斯核)为s，sigma&#x3D;sqrt(s(s+2))&#x2F;2，sigma与核子s尺寸线性相关。<br>halcon 中Sigma的值与mask核有特殊的一一对应</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ksize sigmal</span><br><span class="line">3    (0.600)</span><br><span class="line">5    (1.075)</span><br><span class="line">7    (1.550)</span><br><span class="line">9    (2.025)</span><br><span class="line">11   (2.550)</span><br></pre></td></tr></table></figure><p>code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> ksize = <span class="number">5</span>;</span><br><span class="line"><span class="type">double</span> sigmal=<span class="number">1.41</span>;</span><br><span class="line"><span class="comment">// halcon</span></span><br><span class="line"><span class="built_in">gaussFilter</span>(TiledImage, &amp;ImageGauss, <span class="number">7</span>);</span><br><span class="line"><span class="comment">// cv</span></span><br><span class="line">cv::<span class="built_in">GaussianBlur</span>(*temp_image, *temp_output_image, <span class="built_in">Size</span>(ksize, ksize), sigmal, sigmal, BORDER_DEFAULT);</span><br></pre></td></tr></table></figure><p>参考&amp;引用：</p><p><a href="https://github.com/JockerLin/CV/blob/master/own/practice/calgauss/calgauss.py">calgauss</a><br><a href="http://www.ruanyifeng.com/blog/2012/11/gaussian_blur.html">高斯模糊的算法</a></p><h4 id="4-双边bilateral"><a href="#4-双边bilateral" class="headerlink" title="4) 双边bilateral"></a>4) 双边bilateral</h4><p>SigmaRange用于根据当前像素周围ImageJoint的像素来修改滤镜掩模。只有对比度低于SigmaRange的弱边缘区域的像素才会对平滑做出贡献。请注意，在uint2或真实图像中的对比度可能与SigmaRange的默认值有很大的不同，请相应调整该参数。</p><p>GenParamName和GenParamValue目前可以用来控制精度和速度之间的权衡（见下文）。</p><p>该滤波器能够保留边缘信息，确保梯度大的地方不会被滤波模糊化。</p><p><img src="/material%5Cpreprosess%5Cbilateral.PNG" alt="bilateral"></p><p>如下图像，第一个噪点10个灰度差，第一个阶梯50灰度差，第二个阶梯100灰度差</p><p><img src="/material%5Cpreprosess%5Cbilateral_anay1.PNG" alt="bilateral_anay1"></p><p>若保留20灰度差以上的梯度进行滤波：</p><p><img src="/material%5Cpreprosess%5Cbilateral_anay2.PNG" alt="bilateral_anay2"></p><p>发现了10个灰度差的噪点被消除(平滑)了，同理，如下滤波阈值是50,100灰度差，滤波阈值为50的图像下，100的阶梯被保留下来，50的阶梯被平滑了；滤波阈值为100的图像下，无论是50、100的阶梯都被平滑化；</p><p><img src="/material%5Cpreprosess%5Cbilateral_anay3.PNG" alt="bilateral_anay3"></p><p><img src="/material%5Cpreprosess%5Cbilateral_anay4.PNG" alt="bilateral_anay4"></p><p>对输入Image和ImageJoint的分析</p><p>三种情况</p><p>ImageJoint可以理解为对输入图像的maskImage，但是不是二值的图像，相当于定义了哪些地方需要模糊，哪些地方需要保留梯度。</p><p>如果Image和ImageJoint是相同的，bilateral_filter的行为就像一个边缘保护平滑，其中SigmaSpatial定义了过滤器掩模的大小。对比度明显大于 SigmaRange 的边缘像素会被保留，而同质区域的像素会被平滑化。</p><p>如果Image和ImageJoint不同，Image的每个像素都会受到ImageJoint的影响，用模板进行平滑处理。ImageJoint有强边缘的位置的像素，其对比度明显大于SigmaRange，其平滑化程度低于同区域的像素。</p><p>如果ImageJoint是常数，bilateral_filter相当于用SigmaSpatial进行高斯平滑（见gauss_filter或smooth_image）。</p><p>原理分析：</p><p>双边滤波器可以看做是空间距离与灰度距离的加权，这个加权比例就是SigmalSpatical与SigmalRange。</p><p>目标像素：<br>$$<br>I^{‘}<em>{i} &#x3D; \frac{1}{K_i}\sum</em>{k \in w_i}C_{ik} \cdot S_{ik} \cdot I_k<br>$$<br>空间距离：<br>$$<br>C_{ik} &#x3D; exp({\frac{-(i-k)^2}{2 \cdot SigmaSpatial^2}})<br>$$<br>灰度距离：<br>$$<br>S_{ik} &#x3D; exp({\frac{-(J_i-J_k)^2}{2 \cdot SigmaRange^2}})<br>$$</p><p>$$<br>I_i与J_i是Image与ImageJoint在i位置的灰度值，w_i是i的邻域<br>$$</p><p>图解：</p><img src="material\preprosess\bilateral_work.jpg" alt="bilateral_work" style="zoom:50%;" /><p>SigmaRange越大，边缘越模糊，极限情况为SigmaRange无穷大，忽略常数时，将近为exp（0）&#x3D; 1，与高斯模板（空间域模板）相乘后可认为等效于高斯滤波。</p><p>SigmaRange越小，边缘越清晰，极限情况为SigmaRange无限接近0，接近exp（-∞） &#x3D; 0，与高斯模板（空间域模板）相乘后，可近似为系数皆相等，等效于源图像。</p><p>code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> ksize = <span class="number">5</span>;</span><br><span class="line"><span class="comment">// halcon</span></span><br><span class="line"><span class="built_in">bilateralFilter</span>(TiledImage, TiledImage, &amp;ImageBilateral, <span class="number">3</span>, <span class="number">40</span>, [], []);</span><br><span class="line"><span class="comment">// 滚动双边滤波器</span></span><br><span class="line"><span class="keyword">for</span> I := <span class="number">1</span> to <span class="number">6</span> by <span class="number">1</span></span><br><span class="line">  <span class="built_in">bilateral_filter</span> (TiledImage, ImageJoint, ImageJoint, <span class="number">25</span>, <span class="number">15</span>, [], [])</span><br><span class="line">endfor</span><br><span class="line"><span class="comment">// cv</span></span><br><span class="line">cv::<span class="built_in">bilateralFilter</span>(</span><br><span class="line">*temp_image,</span><br><span class="line">*temp_output_image,</span><br><span class="line">ksize,</span><br><span class="line">bilateral_sigmal_color,</span><br><span class="line">bilateral_sigmal_space);</span><br></pre></td></tr></table></figure><p>参考&amp;引用：</p><p><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html">Bilateral Filtering for Gray and Color Images</a><br><a href="https://www.mvtec.com/doc/halcon/13/en/bilateral_filter.html">Halcon bilateral_filter (Operator)</a><br><a href="https://blog.csdn.net/Jfuck/article/details/8932978">双边滤波（Bilateral Filter）详解</a></p><h4 id="5-二项binomial"><a href="#5-二项binomial" class="headerlink" title="5) 二项binomial"></a>5) 二项binomial</h4><p>使用掩膜大小为MaskWidth * MaskHeight像素的二项式滤镜平滑图像，并返回平滑后图像。 二项式滤波器只需使用整数运算即可非常高效地实现近似高斯滤波器的效果。该滤波效果只与核尺寸相关。</p><p>二项式定义：<br>$$<br>{\displaystyle (x+y)^{4};&#x3D;;x^{4},+,4x^{3}y,+,6x^{2}y^{2},+,4xy^{3},+,y^{4}.}<br>$$</p><p>$$<br>{\displaystyle ax^{b}y^{c}} \texttt{称作二项式系数，记做} {\displaystyle {\tbinom {n}{b}}} or {\displaystyle {\tbinom {n}{a}}}<br>$$</p><p>$$<br>{\tbinom {n}{a}} &#x3D; \frac{n!}{a!(n-a)!}<br>$$</p><p>$$<br>\texttt{位置i,j,核宽高mn像素值：} b_{ij}&#x3D;\frac{1}{2^{n+m-2}}{\tbinom {m-1}{i}}{\tbinom {n-1}{j}}<br>$$</p><p>举例：以ksize&#x3D;3的卷积核为：</p><p><img src="/material%5Cpreprosess%5Cbinomial_kernel.PNG" alt="binomial_kernel"></p><p>code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> ksize = <span class="number">5</span>;</span><br><span class="line"><span class="comment">// halcon</span></span><br><span class="line"><span class="built_in">binomialFilter</span>(TiledImage, ImageBinomial1, ksize, ksize);</span><br><span class="line"><span class="comment">// cv</span></span><br><span class="line"><span class="comment">// diy</span></span><br></pre></td></tr></table></figure><h4 id="6-对比图像"><a href="#6-对比图像" class="headerlink" title="6) 对比图像"></a>6) 对比图像</h4><p>以四种噪声为输入图像</p><p>原图：</p><img src="material\preprosess\原图.bmp" alt="原图" style="zoom: 33%;" /><p>噪声图像：</p><p>从左到右分别是：白噪声、椒盐噪声、均值噪声、高斯噪声</p><p><img src="/material%5Cpreprosess%5Cnoise.bmp" alt="noise"></p><p>用以上预处理方法做横向对比， 合成图像尺寸4096X650，查看输出图像：</p><p>均值 1ms：</p><p><img src="/material%5Cpreprosess%5C%E5%B9%B3%E5%9D%87.bmp" alt="平均"></p><p>中值 28ms：</p><p><img src="/material%5Cpreprosess%5C%E4%B8%AD%E5%80%BC.bmp" alt="中值"></p><p>高斯 3ms：</p><p><img src="/material%5Cpreprosess%5Cgauss.bmp" alt="gauss"></p><p>双边 200ms：</p><p><img src="/material%5Cpreprosess%5C%E5%8F%8C%E8%BE%B9.bmp" alt="双边"></p><p>二项：2ms</p><p><img src="/material%5Cpreprosess%5C%E4%BA%8C%E9%A1%B9.bmp" alt="二项"></p><h4 id="7-综合分析"><a href="#7-综合分析" class="headerlink" title="7) 综合分析"></a>7) 综合分析</h4><p>均值方法能抑制均值噪声，中值滤波对椒盐噪声的过滤效果明显，双边滤波能够保持边缘特性，同时平滑某梯度阈值下的图像，高斯滤波与二项滤波类似，产生中心占比大两边占比小的滤波效果。</p><p>实际使用过程中先判断噪声类型予以选择合适方法。</p><h3 id="OTSU"><a href="#OTSU" class="headerlink" title="OTSU"></a>OTSU</h3><p>OTSU work:</p><p><a href="https://zhuanlan.zhihu.com/p/95034826">参考</a></p><p>通过最大化类间方差计算阈值，能够最好的分割前景和背景。</p><p>历遍每一个灰度值</p><p>定义：<br>$$<br>g &#x3D; w_0*(u_0-u)^2+w_1*(u_1-u)^2\<br>u0 &#x3D; n0灰度累加和&#x2F;n0\<br>g:类间方差（那个灰度的g最大，哪个灰度就是需要的阈值t）\<br>u0：前景平均灰度\<br>u1：背景平均灰度\<br>t ：灰度阈值（我们要求的值，大于这个值的像素我们将它的灰度设置为255，小于的设置为0）\<br>n0：小于阈值的像素，前景\<br>n1：大于等于阈值的像素，背景\<br>w0：前景像素数量占总像素数量的比例 \<br>w1：背景像素数量占总像素数量的比例 \<br>$$</p><p>$$<br>u &#x3D; w_0 * u_0 + w_1 * u_1\<br>g &#x3D; w_0 * w_1 * (u_0 - u_1) ^ 2<br>$$</p><p><a href="https://github.com/JockerLin/CV/blob/master/own/practice/threshold/otsu.py">github代码:otsu.py</a></p><h3 id="卷积算子-高斯、sobel"><a href="#卷积算子-高斯、sobel" class="headerlink" title="卷积算子(高斯、sobel)"></a>卷积算子(高斯、sobel)</h3><p>Gauss:</p><h3 id="区域生长"><a href="#区域生长" class="headerlink" title="区域生长"></a>区域生长</h3><h3 id="分水岭"><a href="#分水岭" class="headerlink" title="分水岭"></a>分水岭</h3><p><img src="/material/watershed.png" alt="watershed"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">Begin[阈值处理] --&gt; O</span><br><span class="line">O[形态学与膨胀,去除视野中的小点点] --&gt; A</span><br><span class="line">A[转换成距离图像dist,每个pixel距离最近的0pixel的距离] --&gt; B</span><br><span class="line">B[二值化dist,阈值为0.7dist.max,pixel==255被当做山峰] --&gt; C</span><br><span class="line">C[不确定区域unknow=膨胀图像O-二值化distB] --&gt; D</span><br><span class="line">C --&gt; E</span><br><span class="line">D[根据山峰B寻找种子markers] --&gt; E</span><br><span class="line">E[设置不确定区域unknow背景] --&gt; F</span><br><span class="line">F[根据原图与种子markers得到边沿]</span><br></pre></td></tr></table></figure><p><a href="https://github.com/JockerLin/CV/blob/master/own/practice/watershed/watershed.py">github代码:watershed.py</a></p><h3 id="HOG-amp-DPM"><a href="#HOG-amp-DPM" class="headerlink" title="HOG &amp; DPM"></a>HOG &amp; DPM</h3><h4 id="HOG"><a href="#HOG" class="headerlink" title="HOG"></a>HOG</h4><ul><li>梯度计算</li><li>单元划分</li><li>区块选择</li><li>区间归一化</li></ul><p><img src="/material/hog_compare.png" alt="hog"></p><p><a href="https://zhuanlan.zhihu.com/p/33059421">参考:图像学习-HOG特征</a></p><p><a href="https://www.learnopencv.com/histogram-of-oriented-gradients/">参考:Histogram of Oriented Gradients</a></p><p><a href="https://zhuanlan.zhihu.com/p/104670289">HOG、LBP 和 Haar 三大特征</a></p><p><img src="/material/histogram_of_gradients.jpg" alt="histogram_of_gradients"></p><p><img src="/material/cell_histogram.jpg" alt="cell_histogram"></p><h5 id="具体分析"><a href="#具体分析" class="headerlink" title="具体分析"></a>具体分析</h5><p>在经过normalize之前，还需要经过一个average的过程，当前的直方向量(9×1)会除一个数，这个数由周围的9个直方数据决定。</p><p>在使用skimage上，[64,128]的img，cell大小为(8,8)，归一化block为(2,2)，<strong>归一化</strong>之后的向量数组为7<em>15</em>2<em>2</em>9&#x3D;3780。</p><p><a href="https://github.com/JockerLin/CV/blob/master/own/practice/hog/hog_release.py">github代码:hog_release.py</a></p><h4 id="DPM-Deformable-part-based-model"><a href="#DPM-Deformable-part-based-model" class="headerlink" title="DPM(Deformable part-based model)"></a>DPM(Deformable part-based model)</h4><p>(VOC07,08,09年的检测冠军)</p><p>分属于目标检测算法,DPM是HOG的拓展。</p><p><img src="/material/hog_vs_dpm.jpg"></p><p>HOG&amp;LInear SVM是直接对一张图片求梯度特征后预测出整个人体的位置信息</p><p>DPM&amp;Latent SVM是使用对人体分部分检测，得到例如头的roi，手的roi，脚、腿roi，根据这些子模块计算与主模块人整体之间的距离量，根据子模块预测出主模块(人体)的位置信息。</p><h3 id="关键点检测"><a href="#关键点检测" class="headerlink" title="关键点检测"></a>关键点检测</h3><h3 id="SIFT-Scale-invariant-feature-transform-尺度不变特征转换"><a href="#SIFT-Scale-invariant-feature-transform-尺度不变特征转换" class="headerlink" title="SIFT(Scale-invariant feature transform 尺度不变特征转换)"></a>SIFT(Scale-invariant feature transform 尺度不变特征转换)</h3><p>SIFT 特征在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。</p><p>不同theta的高斯模糊相减能得到关键点信息，利用高斯金字塔的每一层获取不同尺寸img的关键点信息。</p><p>主要流程：<br>1、构造DOG尺度空间<br>2、关键点搜索与定位<br>3、关键点方向<br>4、关键点描述<br>5、特征匹配</p><p><strong>DOG尺度空间</strong><br>一、定义<br>尺度空间中各尺度的图像模糊程度逐渐增大，尺度越大，图像越模糊。<br>模糊的方法联系高斯模糊的知识点：为了让尺度体现其连续性，高斯金字塔在简单降采样的基础上加上了高斯滤波。将图像金字塔每层的一张图像使用不同参数做高斯模糊，使得金字塔的每层含有多张高斯模糊图像，将金字塔每层多张图像合称为一组(Octave)，金字塔每层只有一组图像，组数和金字塔层数相等。</p><p>Lindeberg等人已证明<strong>高斯卷积核是实现尺度变换的唯一变换核</strong>，并且是唯一的线性核。</p><p>二、哪些是要查找的特征点？<br>目的是要不同尺度图像下都能检测出的点，所以，这些点是具有方向信息的局部极值点。不会受尺寸、光照、旋转等因素的影响，例如角点、边缘、暗区域的亮点与亮区域的暗点。</p><p>三、多尺度多分辨率<br>尺度空间与金字塔的区别：<br>尺度空间的图像系列是由不同高斯核卷积得到，细节逐步丢失，具有相同的分辨率；<br>金字塔采用抽层降采样的方法，每层分辨率成倍减少；</p><p>四、<strong>DOC差分金字塔</strong><br><img src="material/doc_pry.jpg" style="zoom: 80%;" /><br>高斯金字塔：对图像做高斯滤波平滑+金字塔降采样<br>各层金字塔img分辨率不同，在每层的金字塔内，使用不同theta值能获取许多相同分辨率的img。<br>（用一维高斯核分别对行列卷积实现加速效果）</p><p>差分图像：对同一层内的不同theta产生的gauss img相减，得到差分图像<br><img src="material/doc_difference_img.jpg" style="zoom:50%;" /></p><p>本例拿一张建筑物的img做参考：<br><img src="material/doc_building_img.png" style="zoom:80%;" /></p><p>可以看到，doc图像提取到一些轮廓特征比原图能够更好地识别到。</p><p><strong>极值点计算</strong></p><img src="material/key_points.jpg" style="zoom:80%;" /><p>关键点是由DOG空间的局部极值点组成的，局部极值点是框选一个roi，判断中心点的像素与周围点的像素差是否都满足在某个范围内。</p><p>每一幅高斯差分差分img中的每一个像素点(图中的X)要与上下层、本层周围的其他26个像素点比较，确保在尺度空间和本层的图像空间能够检测到极值点。</p><p>具体步骤：</p><p>1、历遍差分金字塔内所有img，获取特定分辨率与尺度下的高斯差分图像$I_{o,s}$；</p><p>2、同组当前层，中心像素满足设定阈值才进入下一步的周围8像素领域比较，若该领域存在中心像素为极大极小值的情况，则认为中心像素是极值点，进行下一步；</p><p>3、同组不同层，在$I_{o,s}$的上下两层高斯差分图像$I_{o,s-1}$与$I_{o,s+1}$中，若上下两层邻域内(18pixel)存在center是相对极值点，则确定该center是本层与上下邻域内的(26pixel)极值点；</p><p>4、将确定极值点的坐标加入集合；</p><p>实验证明：在s0(&#x3D;3)组内层数的尺度下获取极值点是比较高效的表现。在极值点比较的过程中，总高斯差分层数为s下，每一副图像的首尾s&#x2F;&#x2F;2层是无法比较极值的，因此需要在每一组图像的顶层用高斯模糊生成s0(3)幅图像，高斯金字塔每组就有s+s0(s+3)图像，差分金字塔每组s+s0-1(s+2)层图像，因此需要牺牲的高斯金字塔层数为0,1,s层。</p><p><strong>抽取稳定关键点</strong></p><p>对极值点筛选，才作为关键点。<br>DoG值对噪声和边缘较敏感</p><p>1、去除低对比度<br>将每个高斯差分图像中的极值点映射回原图像的像素上，判断是否满足阈值要求。</p><p>2、去除边缘相应点</p><h4 id="ORB"><a href="#ORB" class="headerlink" title="ORB"></a>ORB</h4><h3 id="HOUGH检测"><a href="#HOUGH检测" class="headerlink" title="HOUGH检测"></a>HOUGH检测</h3><p><a href="https://www.cnblogs.com/kk17/p/9693132.html#%E9%9C%8D%E5%A4%AB%E6%A2%AF%E5%BA%A6%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8D%B3%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E4%B8%A4%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E8%A7%A3%E9%87%8A">参考:霍夫梯度法的原理</a></p><h4 id="直线"><a href="#直线" class="headerlink" title="直线"></a>直线</h4><h5 id="霍夫空间"><a href="#霍夫空间" class="headerlink" title="霍夫空间"></a>霍夫空间</h5><p>一般直线方程为y&#x3D;kx+b</p><p>将x，y坐标空间转到k，b坐标空间，在xy空间的一条线L1，在kb空间就相当于一个点P1，因为L1上的点拥有相同的kb值，而在kb空间的一条线L2，可以看做在xy空间下的一点P2，L2上的每个点都是xy中过点P2的一条直线。</p><p>即xy的一般Hough方程是：b&#x3D;-kx+y</p><p>为了避免许多奇异点的出现，将方程改写如下：</p><p>$$<br>p &#x3D; xcos\theta + ysin\theta<br>$$</p><p>表示了x、y坐标空间到Θ、P代表的HOUGH空间的转换。</p><h5 id="标准霍夫线变换算法流程"><a href="#标准霍夫线变换算法流程" class="headerlink" title="标准霍夫线变换算法流程"></a>标准霍夫线变换算法流程</h5><ol><li>读取原始图像，并转换成灰度图，利用阈值分割或者边缘检测算子转换成二值化边缘图像</li><li>初始化霍夫空间， 令所有𝑁𝑢𝑚(𝜃,𝑝)&#x3D;0</li><li>对于每一个像素点(𝑥,𝑦)，在参数空间中找出所有满足𝑥𝑐𝑜𝑠𝜃+𝑦𝑠𝑖𝑛𝜃&#x3D;𝑝的(𝜃,𝑝)对,然后令𝑁𝑢𝑚(𝜃,𝑝)&#x3D;𝑁𝑢𝑚(𝜃,𝑝)+1</li><li>统计所有𝑁𝑢𝑚(𝜃,𝑝)的大小，取出𝑁𝑢𝑚(𝜃,𝑝)&gt;τ的参数（τ是所设的阈值），从而得到一条直线。</li><li>将上述流程取出的直线，确定与其相关线段的起始点与终止点（有一些算法，如蝴蝶形状宽度，峰值走廊之类）</li></ol><h5 id="统计概率霍夫变换算法流程"><a href="#统计概率霍夫变换算法流程" class="headerlink" title="统计概率霍夫变换算法流程"></a>统计概率霍夫变换算法流程</h5><p>标准霍夫变换本质上是把图像映射到它的参数空间上，它需要计算所有的M个边缘点，这样它的运算量和所需内存空间都会很大。如果在输入图像中只是处理𝑚(𝑚&lt;𝑀)个边缘点，则这m个边缘点的选取是具有一定概率性的，因此该方法被称为概率霍夫变换（Probabilistic Hough Transform）。</p><p>该方法还有一个重要的特点就是能够检测出线端，即能够检测出图像中直线的两个端点，确切地定位图像中的直线。HoughLinesP函数就是利用概率霍夫变换来检测直线的。它的一般步骤为：</p><ol><li>随机抽取图像中的一个特征点，即边缘点，如果该点已经被标定为是某一条直线上的点，则继续在剩下的边缘点中随机抽取一个边缘点，直到所有边缘点都抽取完了为止；</li><li>对该点进行霍夫变换，并进行累加和计算；</li><li>选取在霍夫空间内值最大的点，如果该点大于阈值的，则进行步骤4，否则回到步骤1；</li><li>根据霍夫变换得到的最大值，从该点出发，沿着直线的方向位移，从而找到直线的两个端点；</li><li>计算直线的长度，如果大于某个阈值，则被认为是好的直线输出，回到步骤1。</li></ol><h4 id="圆"><a href="#圆" class="headerlink" title="圆"></a>圆</h4><h5 id="经典hough圆"><a href="#经典hough圆" class="headerlink" title="经典hough圆"></a>经典hough圆</h5><p>霍夫圆变换和霍夫线变换的原理类似。霍夫线变换是两个参数(r,θ)，霍夫圆需要三个参数，圆心的x,y坐标和圆的半径,他的方程表达式为(𝑥−𝑎)2+(𝑦−𝑏)2&#x3D;𝑐2,按照标准霍夫线变换思想，在xy平面，三个点在同一个圆上，则它们对应的空间曲面相交于一点（即点(a,b,c))。故我们如果知道一个边界上的点的数目，足够多，且这些点与之对应的空间曲面相交于一点。则这些点构成的边界，就接近一个圆形。上述描述的是标准霍夫圆变换的原理，由于三维空间的计算量大大增大的原因, 标准霍夫圆变化很难被应用到实际中。</p><img src="/home/pilcq/Nutstore Files/BlazarLin/Notes/material/houghcircle_std.jpeg" style="zoom: 80%;" /><h5 id="hough梯度法"><a href="#hough梯度法" class="headerlink" title="hough梯度法"></a>hough梯度法</h5><p>第一阶段用于检测圆心，第二阶段从圆心推导出圆半径。</p><p>version 1:</p><p>2-1霍夫变换的具体步骤为：</p><ol><li><p>首先对图像进行边缘检测，调用opencv自带的cvCanny()函数，将图像二值化，得到边缘图像。</p></li><li><p>对边缘图像上的每一个非零点。采用cvSobel()函数，计算x方向导数和y方向的导数，从而得到梯度。从边缘点，沿着梯度和梯度的反方向，对参数指定的min_radius到max_radium的每一个像素，在累加器中被累加。同时记下边缘图像中每一个非0点的位置。</p></li><li><p>从（二维）累加器中这些点中选择候选中心。这些中心都大于给定的阈值和其相邻的四个邻域点的累加值。</p></li><li><p>对于这些候选中心按照累加值降序排序，以便于最支持的像素的中心首次出现。</p></li><li><p>对于每一个中心，考虑到所有的非0像素（非0，梯度不为0），这些像素按照与其中心的距离排序，从最大支持的中心的最小距离算起，选择非零像素最支持的一条半径。</p></li><li><p>如果一个中心受到边缘图像非0像素的充分支持，并且到前期被选择的中心有足够的距离。则将圆心和半径压入到序列中，得以保留。</p></li></ol><p>version 2:</p><p>第一阶段：检测圆心</p><ol><li><p>对输入图像边缘检测；</p></li><li><p>计算图形的梯度，并确定圆周线，其中圆周的梯度就是它的法线；</p></li><li><p>在二维霍夫空间内，绘出所有图形的梯度直线，某坐标点上累加和的值越大，说明在该点上直线相交的次数越多，也就是越有可能是圆心；(备注：这只是直观的想法，实际源码并没有划线)</p></li><li><p>在霍夫空间的4邻域内进行非最大值抑制；</p></li><li><p>设定一个阈值，霍夫空间内累加和大于该阈值的点就对应于圆心。</p></li></ol><p>第二阶段：检测圆半径</p><ol><li><p>计算某一个圆心到所有圆周线的距离，这些距离中就有该圆心所对应的圆的半径的值，这些半径值当然是相等的，并且这些圆半径的数量要远远大于其他距离值相等的数量</p></li><li><p>设定两个阈值，定义为最大半径和最小半径，保留距离在这两个半径之间的值，这意味着我们检测的圆不能太大，也不能太小</p></li><li><p>对保留下来的距离进行排序</p></li><li><p>找到距离相同的那些值，并计算相同值的数量</p></li><li><p>设定一个阈值，只有相同值的数量大于该阈值，才认为该值是该圆心对应的圆半径</p></li><li><p>对每一个圆心，完成上面的2.1～2.5步骤，得到所有的圆半径</p></li></ol><h3 id="角点检测"><a href="#角点检测" class="headerlink" title="角点检测"></a>角点检测</h3><p>Harris 角点检测算法如下：</p><p>1、对图像进行灰度化处理；</p><p>2、利用Sobel滤波器求出海森矩阵（Hessian matrix）这一步的数学原理可见<a href="https://blog.csdn.net/lwzkiller/article/details/54633670">这里</a>。<br>$$<br>H&#x3D;\left[\begin{matrix}{I_x}^2&amp;I_xI_y\I_xI_y&amp;<br>{I_y}^2\end{matrix}\right]将高斯滤波器分别应用于<br>$$<br>$$<br>{I_x}^2、{I_y}^2、I_x\ I_y<br>$$<br>3、计算每个像素的<br>$$<br>R &#x3D; \det(H) - k\ (\text{trace}(H))^2\<br>\det(H)<br>$$<br>通常K在[0.04,0.16]范围内取值.</p><p>4、满足<br>$$<br>R \geq \max(R) \cdot\text{th}<br>$$</p><p>的像素点即为角点。</p><p><a href="https://github.com/JockerLin/ImageProcessing100Wen/blob/my_analysis/Question_81_90/answers/answer_83.py">代码在这里</a><br><a href="https://github.com/gzr2017/ImageProcessing100Wen/blob/master/Question_81_90/README.pdf">参考</a></p><h3 id="插值方法"><a href="#插值方法" class="headerlink" title="插值方法"></a>插值方法</h3><p>已转移。</p><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><p>直方图表示了一副图像的像素分布，数据集中在直方图左侧整体就会偏暗，右侧偏亮，直方图的偏向导致了图像的动态范围就比较低。使直方图较为平坦能够让人更清楚的看清图片。</p><h3 id="利用直方图，进行图像识别"><a href="#利用直方图，进行图像识别" class="headerlink" title="利用直方图，进行图像识别"></a>利用直方图，进行图像识别</h3><p>图像识别是识别图像中物体的类别（它属于哪个类）的任务。图像识别通常被称为Classification、Categorization、Clustering等。</p><p>一种常见的方法是通过 HOG、SIFT、SURF 等方法从图像中提取一些特征，并通过特征确定物体类别。这种方法在CNN普及之前广泛采用，但CNN可以完成从特征提取到分类等一系列任务。</p><p>这里，利用图像的颜色直方图来执行简单的图像识别。算法如下：</p><ol><li>将图像<code>train_***.jpg</code>进行减色处理（像问题六中那样，RGB取4种值）。</li><li>创建减色图像的直方图。直方图中，RGB分别取四个值，但为了区分它们，B &#x3D; [1,4]、G &#x3D; [5,8]、R &#x3D; [9,12]，这样bin&#x3D;12。请注意，我们还需要为每个图像保存相应的柱状图。也就是说，需要将数据储存在<code>database = np.zeros((10(训练数据集数), 13(RGB + class), dtype=np.int)</code>中。</li><li>将步骤2中计算得到的柱状图记为 database。</li><li>计算想要识别的图像<code>test@@@.jpg</code>与直方图之间的差，将差称作特征量。</li><li>直方图的差异的总和是最小图像是预测的类别。换句话说，它被认为与近色图像属于同一类。</li><li>计算将想要识别的图像（<code>test_@@@.jpg</code>）的柱状图（与<code>train_***.jpg</code>的柱状图）的差，将这个差作为特征量。</li><li>统计柱状图的差，差最小的图像为预测的类别。换句话说，可以认为待识别图像与具有相似颜色的图像属于同一类。</li></ol><img src="material/simple_img_cls.png" style="zoom:50%;" /><p>训练数据集存放在文件夹<code>dataset</code>中，分为<code>trainakahara@@@.jpg</code>（类别1）和<code>trainmadara@@@.jpg</code>（类别2）两类，共计10张。<code>akahara</code>是红腹蝾螈（Cynops pyrrhogaster），<code>madara</code>是理纹欧螈（Triturus marmoratus）。</p><p>这种预先将特征量存储在数据库中的方法是第一代人工智能方法。这个想法是逻辑是，如果你预先记住整个模式，那么在识别的时候就没有问题。但是，这样做会消耗大量内存，这是一种有局限的方法。</p><h4 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h4><p>如果⽐较这两个图像，它们绿⾊和⿊⾊⽐例看起来差不多，因此整个图像颜⾊看起来相同。这是因为在识别的时候，训练图像选择了⼀张偏离⼤部分情况的图像。因此，训练数据集的特征不能很好地分离，并且有时包括偏离特征分布的样本。<br>为了避免这中情况发⽣，在这⾥我们选择颜⾊相近的三副图像，并通过投票来预测最后的类别，再计算正确率。像这样选择具有相似特征的3个学习数据的⽅法被称为 k-近邻算法（k-NN: k-Nearest Neighbor）。</p><p><a href="https://github.com/JockerLin/ImageProcessing100Wen/blob/my_analysis/Question_81_90/answers/answer_87.py">代码在这里</a><br><a href="https://github.com/gzr2017/ImageProcessing100Wen/blob/master/Question_81_90/README.pdf">参考</a></p><h4 id="k-平均聚类算法（k-means-Clustering）"><a href="#k-平均聚类算法（k-means-Clustering）" class="headerlink" title="k-平均聚类算法（k -means Clustering）"></a>k-平均聚类算法（k -means Clustering）</h4><p>k-平均聚类算法在类别数已知时使用。在质心不断明确的过程中完成特征量的分类任务。</p><p>k-平均聚类算法如下：</p><ol><li>为每个数据随机分配类；</li><li>计算每个类的重心；</li><li>计算每个数据与重心之间的距离，将该数据分到重心距离最近的那一类；</li><li>重复步骤2和步骤3直到没有数据的类别再改变为止。</li></ol><p>在这里，以减色化和直方图作为特征量来执行以下的算法：</p><ol><li>对图像进行减色化处理，然后计算直方图，将其用作特征量；</li><li>对每张图像随机分配类别0或类别1（在这里，类别数为2，以<code>np.random.seed (1)</code>作为随机种子生成器。当<code>np.random.random</code>小于<code>th</code>时，分配类别0；当<code>np.random.random</code>大于等于<code>th</code>时，分配类别1，在这里<code>th=0.5</code>）；</li><li>分别计算类别0和类别1的特征量的质心（质心存储在<code>gs = np.zeros((Class, 12), dtype=np.float32)</code>中）；</li><li>对于每个图像，计算特征量与质心之间的距离（在此取欧氏距离），并将图像指定为质心更接近的类别。</li><li>重复步骤3和步骤4直到没有数据的类别再改变为止。</li></ol><p>在这里，实现步骤1至步骤3吧（步骤4和步骤5的循环不用实现）！将图像<code>test@@@.jpg</code>进行聚类。</p><p><strong>计算质心</strong>&#x3D;&gt;<strong>聚类</strong>&#x3D;&gt;<strong>调整初期类别</strong></p><p><a href="https://github.com/JockerLin/ImageProcessing100Wen/blob/my_analysis/Question_81_90/answers/answer_90.py">代码在这里</a><br><a href="https://github.com/gzr2017/ImageProcessing100Wen/blob/master/Question_81_90/README.pdf">参考</a></p><h3 id="linemod-模板匹配"><a href="#linemod-模板匹配" class="headerlink" title="linemod 模板匹配"></a>linemod 模板匹配</h3><p>linemod:</p><p>通过选择一个模板图像，训练，旋转与缩放得到许多的候选模板图，特征点的方法另外阐述；</p><p>目标图像中匹配模板图得到最佳的一个匹配模板，完成匹配。</p><p>特征点的选取与计算：</p><h3 id="神经网络的搭建与原理"><a href="#神经网络的搭建与原理" class="headerlink" title="神经网络的搭建与原理"></a>神经网络的搭建与原理</h3><h4 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h4><p>看完这个在进行其他神经网络的学习就会发现是换汤不换药。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">Begin[开始] --&gt; O</span><br><span class="line">O[定义网络的模型,包含层数,激励函数,损失函数] --&gt; A</span><br><span class="line">A[准备训练数据与对应的label] --&gt; B</span><br><span class="line">B[模型开始训练] --&gt; C</span><br><span class="line">C[每训练一次,根据loss,更新一次内部层的参数值] --&gt; D[满足了训练次数]</span><br><span class="line">D--Yes--&gt;F</span><br><span class="line">D--No--&gt;C</span><br><span class="line">F[将测试集输入模型,得到模型预测的label_predict] --&gt; G</span><br><span class="line">G[将预测的label_predict与真实的label进行比较,得到测试的误差] --&gt; H</span><br><span class="line">H[误差分析或结束训练] --&gt; I</span><br><span class="line">I[结束]</span><br></pre></td></tr></table></figure><p><a href="https://github.com/JockerLin/ImageProcessing100Wen/blob/my_analysis/Question_81_90/answers/answer_95.py">代码在这里</a><br><a href="https://github.com/gzr2017/ImageProcessing100Wen/blob/master/Question_91_100/README.pdf">参考</a></p><h4 id="利用神经网络对图像HOG特征进行学习-分辨蝾螈的头"><a href="#利用神经网络对图像HOG特征进行学习-分辨蝾螈的头" class="headerlink" title="利用神经网络对图像HOG特征进行学习,分辨蝾螈的头"></a>利用神经网络对图像HOG特征进行学习,分辨蝾螈的头</h4><h5 id="涉及要点"><a href="#涉及要点" class="headerlink" title="涉及要点"></a>涉及要点</h5><p>IOU、HOG、NMS</p><p><a href="https://github.com/JockerLin/ImageProcessing100Wen/blob/my_analysis/Question_81_90/answers/answer_100.py">代码在这里</a><br><a href="https://github.com/gzr2017/ImageProcessing100Wen/blob/master/Question_91_100/README.pdf">参考</a></p><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p><a href="http://blog.pluskid.org/?page_id=683">参考</a></p><h3 id="标定程序设计与接口"><a href="#标定程序设计与接口" class="headerlink" title="标定程序设计与接口"></a>标定程序设计与接口</h3><p>todo:delete ３Ｄ相关的已经归纳到专门一篇</p><h3 id="双目成像系统"><a href="#双目成像系统" class="headerlink" title="双目成像系统"></a>双目成像系统</h3><h4 id="行对准图像"><a href="#行对准图像" class="headerlink" title="行对准图像"></a>行对准图像</h4><p>先确定相机的安装方式，踩过坑，相机视野的上下左右是否正确，这决定了反畸变后两幅图像是否能行对准。</p><p>在确定了相机的安装方式后，标定相机，得到内参与畸变系数(k1, k2, p1, p2, [k3, [k4, k5, k6]])。</p><p>使用stereoRectify对双目进行优化，实现行对准，得到矫正后新的相机内参。</p><p>利用新的相机内参计算undistort反畸变图像，看目标时候会超出图像区域，此时的目标先用一张标定时候的图像做参考。</p><!-- <img src="material/left_img_pk_undis_left.jpg" style="zoom:50%;" /> --><p>左：原始图像，右：反畸变图像</p><p><img src="/material/left_img_pk_undis_left.jpg" alt="左：原始图像，右：反畸变图像"></p><p>在使用initUndistortRectifyMap计算矫正先后的map。</p><p>矫正前后图像明显看到是否有<strong>行对准</strong>。</p><p><img src="/material/left_right_img.jpg" alt="左右原始图像"></p><p><img src="/material/left_right_rectify_img.jpg" alt="左右矫正图像"></p><h4 id="生成深度图"><a href="#生成深度图" class="headerlink" title="生成深度图"></a>生成深度图</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Image-Process&quot;&gt;&lt;a href=&quot;#Image-Process&quot; class=&quot;headerlink&quot; title=&quot;Image Process&quot;&gt;&lt;/a&gt;Image Process&lt;/h1&gt;&lt;!-- TOC --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a hre</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>git—分布式版本控制工具</title>
    <link href="https://jockerlin.github.io/2023/05/27/git_skills/"/>
    <id>https://jockerlin.github.io/2023/05/27/git_skills/</id>
    <published>2023-05-27T10:21:23.715Z</published>
    <updated>2023-05-27T12:07:05.288Z</updated>
    
    <content type="html"><![CDATA[<h1 id="git—分布式版本控制工具"><a href="#git—分布式版本控制工具" class="headerlink" title="git—分布式版本控制工具"></a>git—分布式版本控制工具</h1><p><img src="https://i.loli.net/2019/09/23/vMsT57Kucfk2QnJ.png" alt="git_process.png"></p><!-- TOC --><ul><li><a href="#1-%E6%9C%AC%E5%9C%B0%E6%93%8D%E4%BD%9C">1. 本地操作</a></li><li><a href="#2-%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86">2. 分支管理</a></li><li><a href="#3-%E5%A4%9A%E4%BA%BA%E5%8D%94%E4%BD%9C">3. 多人協作</a></li><li><a href="#4-%E6%A8%99%E7%B1%A4%E7%AE%A1%E7%90%86">4. 標籤管理</a></li><li><a href="#5-%E9%81%A0%E7%A8%8B%E5%BA%AB%E7%AE%A1%E7%90%86">5. 遠程庫管理</a><ul><li><a href="#51-%E6%9C%AC%E5%9C%B0%E4%B8%8E%E8%BF%9C%E7%A8%8B%E5%BA%93%E5%85%B3%E8%81%94%E7%9A%84%E6%96%B9%E6%B3%95">5.1. 本地与远程库关联的方法</a></li></ul></li><li><a href="#6-%E6%89%93%E5%8C%85">6. 打包</a></li><li><a href="#7-%E8%BD%A6%E7%A5%B8%E7%8E%B0%E5%9C%BA">7. 车祸现场</a></li><li><a href="#8-%E8%87%AA%E5%AE%9A%E4%B9%89git">8. 自定义git</a><ul><li><a href="#81-%E9%85%8D%E7%BD%AE%E5%88%AB%E5%90%8D">8.1. 配置别名</a></li></ul></li></ul><!-- /TOC --><h2 id="1-本地操作"><a href="#1-本地操作" class="headerlink" title="1. 本地操作"></a>1. 本地操作</h2><p>1.廖雪峯 git Git是目前世界上最先进的分布式版本控制系统</p><p>2.git config的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置</p><p>3.the function of git commit -m “xxx”?? 版本修改了什麼東西 備註</p><p>$ git log显示从最近到最远的提交日志历史</p><p>$ git reset –hard HEAD^回退到上一个版本</p><p>$ git reset –hard XX(版本號)回退到XXX版本</p><p>$ git reflog用来记录你的每一次</p><p>HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用git reset –hard commit_id<br>要重返未来，用git reflog查看历史，以便确定要回到未来的哪个版本</p><p>4.stage(暫存區)的概念 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git add -&gt; git commit</p><p>5.Python編譯器:VIM Emacs Kate</p><p>總結：<br>cd 切換到當前目錄<br>$ git init 把這個目錄變成git可以管理的倉庫<br>(ls -ah 查看隱藏文件)</p><p>$ git add readme.txt 把文件添加到倉庫</p><p>$ git commit -m “xxx（說明的話）” 把文件提交到倉庫附上本次提交的說明</p><p>$ git status 掌握當前庫的狀態</p><p>$ git diff readme.txt 查看修改前後的difference<br>再次修改後進行 $ git add xx 與 $ git commit -m”xxx” 進行庫的更行</p><p>$ git log 查看提交日誌，也可查看版本號 輸入q退出查看</p><p>$ git reset –hard HEAD^ 回退到上一個版本（HEAD表示當前版本，HEAD^<br>表示上一個版本，HEAD^^表示上上個版本，HEAD～100表示往上100個版本）</p><p>$ git reset –hard 版本號  回到版本號對應的版本</p><p>$ git reflog用來記錄每一次</p><p>$ git stash  当前分支工作一半，需要切换分支，当前分支不想commit，进行储藏操作</p><p>$ git stash apply 恢复储藏的内容</p><p>$ git stash list 查看储藏的条目内容</p><p>$ git diff commit-id1 commit-id2 查看两个提交版本id的修改记录差异</p><p>$ git diff commit-id1 commit-id2 –stat 查看两个提交版本id修改了那些文件，可以使用</p><p>$ git commit –amend -m “new information” 覆盖上次提交的信息，即上次commit的信息会消失。</p><p>$ git config –global -l 查看本地全局配置</p><h2 id="2-分支管理"><a href="#2-分支管理" class="headerlink" title="2. 分支管理"></a>2. 分支管理</h2><p>$ git checkout – readme.txt 撤銷修改</p><p>$ git reset HEAD file 把暫存區的修改撤銷掉，用於$ git add後，想要撤銷到工作區</p><p>$ rm test.txt刪除文件 後①從版本庫中刪除 $ git rm test.txt 或②把誤刪的文件恢復到最新版本 $ git checkout – test.txt</p><p>github<br>$ git remote add origin <a href="mailto:&#x67;&#x69;&#116;&#64;&#x67;&#x69;&#x74;&#104;&#117;&#98;&#46;&#x63;&#111;&#109;">&#x67;&#x69;&#116;&#64;&#x67;&#x69;&#x74;&#104;&#117;&#98;&#46;&#x63;&#111;&#109;</a>:JockerLin&#x2F;learngit.git關聯我的遠程庫</p><p>$ git pull –base origin master</p><p>$ git push -u origin master首次推送master分支的內容</p><p>$ git push origin master以後同步推送最新修改</p><p>$ git clone <a href="mailto:&#x67;&#x69;&#116;&#64;&#103;&#105;&#x74;&#104;&#117;&#98;&#x2e;&#x63;&#111;&#109;">&#x67;&#x69;&#116;&#64;&#103;&#105;&#x74;&#104;&#117;&#98;&#x2e;&#x63;&#111;&#109;</a>:JockerLin&#x2F;xx(倉庫名).git從雲端遠程庫同步到本地</p><p>$ git branch 查看分支</p><p>$ git branch <name> 創建分支</p><p>$ git checkout <name> 切換分支</p><p>$ git checkout -b <name> 創建並切換分支</p><p>$ git merge <name> 合併某分支到當前分支(fast forward的合併方式)</p><p>$ git branch -d <name> 刪除分支</p><p>$ git log 查看分支歷史</p><p>$ git log –graph –pretty&#x3D;oneline –abbrev-commit 查看分支的合併情況，包含分製圖、一行顯示、提交驗證碼</p><p>$ git merge –no-ff -m “merge with no-ff” dev 禁用fast forward 普通模式合併，普通合併合併后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 並沒有看出來??????<br>bug分支測試失敗??????<br>feature分支 开发一个新feature，最好新建一个分支；<br>如果要丢弃一个没有被合并过的分支，可以通过git branch -D <name>强行删除<br>多人協作失敗，根據例程，helloworld.py同步成功，但是readme.txt同步失敗??????</p><h2 id="3-多人協作"><a href="#3-多人協作" class="headerlink" title="3. 多人協作"></a>3. 多人協作</h2><p>1、$ git remote -v 查看遠程庫信息</p><p>2、$ git checkout -b XXX(branch-name) origin&#x2F;xxx(branch-name)建立本地分支應與遠程分支名稱一致</p><p>3、$ git branch –set-upstream xxx(branch-name) origin&#x2F;xxx(branch-name)建立本地分支與遠程分支的關聯</p><p>4、$ git push origin xxx(branch-name)推送自己的修改；</p><p>5、若推送失敗，因爲遠程分支版本比本地新，需要先用$ git pull試圖合併；或$ git pull –rebase origin master</p><p>6、合併有衝突，解決衝突後在本地提交；</p><p>7、解決後無衝突，再用步驟1則推送成功；<br>（若$ git pull提示”no tracking information”，應先建立本地分支與遠程分支的鏈接關係$ git branch –set-upstream branch-name origin&#x2F;branch-name）</p><h2 id="4-標籤管理"><a href="#4-標籤管理" class="headerlink" title="4. 標籤管理"></a>4. 標籤管理</h2><p>$ git tag xxx&lt;標簽名&gt; 新建標籤 默認爲HEAD<br>$ git tag xxx&lt;標簽名&gt; xxx<commit id>   新建標籤<br>$ git tag 查看所有標籤<br>$ git tag -a xxx&lt;標簽名&gt; -m “xxx&lt;標籤信息&gt;”<br>$ git show xxx&lt;標簽名&gt; 查看標籤信息<br>$ git push origin xxx&lt;標簽名&gt; 推送一個本地標籤<br>$ git push origin –tags 推送全部未接受過的本地標籤<br>$ git tag -d xxx&lt;標簽名&gt; 刪除本地標籤<br>$ git push origin :refs&#x2F;tags&#x2F;xxx&lt;標簽名&gt; 刪除一個遠程標籤</p><p>錯誤修改 git add .</p><h2 id="5-遠程庫管理"><a href="#5-遠程庫管理" class="headerlink" title="5. 遠程庫管理"></a>5. 遠程庫管理</h2><p>$ git remote -v  查看遠程庫信息<br>$ git remote rm origin  刪除已有的遠程庫</p><p>創建刪除新用戶<br>$ sudo adduser xxx 在home目錄下添加一個賬號<br>$ sudo useradd xxx 僅添加普通用戶，不會再home目錄下添加賬號<br>$ sudo useadd -g root 用戶名 &#x2F;* 讓剛剛建立的用戶劃分到root權限組下<br>$ sudo usedel -r newuser 刪除名爲newuser的用戶</p><p>每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。<br>$ config –global user.name “Your Name”<br>$ git config –global user.email “<a href="mailto:&#x65;&#109;&#97;&#x69;&#108;&#x40;&#x65;&#120;&#97;&#x6d;&#112;&#108;&#x65;&#46;&#x63;&#x6f;&#109;">&#x65;&#109;&#97;&#x69;&#108;&#x40;&#x65;&#120;&#97;&#x6d;&#112;&#108;&#x65;&#46;&#x63;&#x6f;&#109;</a>“<br>注意git config的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。</p><p>$ git remote show origin 查看远程库的origin的信息</p><p>$ git branch -a 查看本地所有分支，红色部分为远程库在本地的copy版本</p><p>$ git remote prune origin 更新本地上的远程库版本（删除不存在的分支）</p><p>$ git remote prune 删除本地版本库上那些失效的远程追踪分支</p><p>$ git branch -vv</p><p>$ git push origin :分支名字 删除远端分支</p><p>$ git push origin –delete 分支名字 删除远端分支</p><p>$ git config –add core.filemode false 忽略文件的chmod修改导致的git diff</p><p>git工程创建后，开发过程中加入.gitignore或更改git配置需要立即生效，则需要清除暂存区staged文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rm -r --cached .</span><br><span class="line">git add .</span><br><span class="line">git commit -m &#x27;update .gitignore&#x27;</span><br></pre></td></tr></table></figure><h3 id="5-1-本地与远程库关联的方法"><a href="#5-1-本地与远程库关联的方法" class="headerlink" title="5.1. 本地与远程库关联的方法"></a>5.1. 本地与远程库关联的方法</h3><p>1、在gitlab或者github新建工程 本地拉取<br>2、远程已经有工程了 本地工程重新整理后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git init #初始化</span><br><span class="line">git remote remove origin # 删除关联的远程库</span><br><span class="line">git add .</span><br><span class="line">git commit -m &#x27;update xxx&#x27;</span><br><span class="line">git remote add origin https://github.com/JockerLin/Notes #重新关联or添加本地库对应的远程库，</span><br><span class="line">git fetch</span><br><span class="line">git merge # 可能需要解决冲突 不同步的问题</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">即可正常push</span></span><br></pre></td></tr></table></figure><p>单独添加远程仓库</p><p>一个本地仓库可以对应多个远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin https://github.com/JockerLin/Notes #重新关联or添加本地库对应的远程库</span><br></pre></td></tr></table></figure><h2 id="6-打包"><a href="#6-打包" class="headerlink" title="6. 打包"></a>6. 打包</h2><h2 id="7-车祸现场"><a href="#7-车祸现场" class="headerlink" title="7. 车祸现场"></a>7. 车祸现场</h2><p>1、</p><p>git push 或 git fetch 的时候 报connect的错误信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pilcq@creater:~/test/VisionTool_PY$ git fetch origin develop:develop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt;fatal: unable to access <span class="string">&#x27;http://192.168.16.210:10080/lu_sa/VisionTool_PY.git/&#x27;</span>: &gt;&gt;&gt;Failed to connect to 127.0.0.1 port 1080: Connection refused</span></span><br></pre></td></tr></table></figure><p>查询后发现可能是翻墙的时候代理没clear干净导致</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pilcq@creater:~/test/VisionTool_PY$ env|grep -i proxy</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt;http_proxy=http://127.0.0.1:1080/</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt;https_proxy=https://127.0.0.1:1080/</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将两个代理清除干净</span></span><br><span class="line">pilcq@creater:~/test/VisionTool_PY$ export http_proxy=&quot;&quot;</span><br><span class="line">pilcq@creater:~/test/VisionTool_PY$ export https_proxy=&quot;&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">再查看</span></span><br><span class="line">pilcq@creater:~/test/VisionTool_PY$ env|grep -i proxy</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt;http_proxy=</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt;https_proxy=</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">清除代理成功，fetch 与 push 恢复正常</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="8-自定义git"><a href="#8-自定义git" class="headerlink" title="8. 自定义git"></a>8. 自定义git</h2><h3 id="8-1-配置别名"><a href="#8-1-配置别名" class="headerlink" title="8.1. 配置别名"></a>8.1. 配置别名</h3><p>用co表示checkout，ci表示commit，br表示branch：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git config --global alias.co checkout</span><br><span class="line">git config --global alias.ci commit</span><br><span class="line">git config --global alias.br branch</span><br><span class="line">git config --global alias.lg &quot;log --color --graph --pretty=format:&#x27;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#x27; --abbrev-commit&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;git—分布式版本控制工具&quot;&gt;&lt;a href=&quot;#git—分布式版本控制工具&quot; class=&quot;headerlink&quot; title=&quot;git—分布式版本控制工具&quot;&gt;&lt;/a&gt;git—分布式版本控制工具&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.n</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>相机标定大全</title>
    <link href="https://jockerlin.github.io/2023/05/27/calibration/"/>
    <id>https://jockerlin.github.io/2023/05/27/calibration/</id>
    <published>2023-05-27T10:21:23.707Z</published>
    <updated>2023-05-27T12:07:26.856Z</updated>
    
    <content type="html"><![CDATA[<h1 id="相机标定大全——平面、单目、双目、眼手"><a href="#相机标定大全——平面、单目、双目、眼手" class="headerlink" title="相机标定大全——平面、单目、双目、眼手"></a>相机标定大全——平面、单目、双目、眼手</h1><p>看完这标定总结你还不懂标定就来打我吧！</p><!-- TOC --><ul><li><a href="#%E4%B8%80%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%87%A0%E4%BD%95%E5%9D%90%E6%A0%87%E6%A6%82%E8%AE%BA">一、机器视觉几何坐标概论</a><ul><li><a href="#1%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E7%B3%BB">1、世界坐标系</a></li><li><a href="#2%E6%91%84%E5%83%8F%E6%9C%BA%E5%9D%90%E6%A0%87%E7%B3%BB">2、摄像机坐标系</a></li><li><a href="#3%E5%9B%BE%E5%83%8F%E5%83%8F%E7%B4%A0%E5%9D%90%E6%A0%87%E7%B3%BB">3、图像（像素）坐标系</a><ul><li><a href="#31%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB">3.1、图像坐标系</a></li><li><a href="#32%E5%83%8F%E7%B4%A0%E5%9D%90%E6%A0%87%E7%B3%BB">3.2、像素坐标系</a></li></ul></li><li><a href="#4%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB">4、坐标系之间的<strong>关系</strong></a><ul><li><a href="#41%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E5%83%8F%E7%B4%A0%E5%9D%90%E6%A0%87%E7%B3%BB">4.1、图像坐标系与像素坐标系</a></li><li><a href="#42%E7%9B%B8%E6%9C%BA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB">4.2、相机坐标系与图像坐标系</a></li><li><a href="#43%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E7%9B%B8%E6%9C%BA%E5%9D%90%E6%A0%87%E7%B3%BB">4.3、世界坐标系与相机坐标系</a></li><li><a href="#44%E4%BB%8E%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E5%88%B0%E5%83%8F%E7%B4%A0%E5%9D%90%E6%A0%87">4.4、从世界坐标到像素坐标</a></li></ul></li></ul></li><li><a href="#%E4%BA%8C%E5%B9%B3%E9%9D%A2%E6%A0%87%E5%AE%9Ahomography%E5%8F%98%E6%8D%A2">二、平面标定（Homography变换）</a><ul><li><a href="#1%E5%AE%9A%E4%B9%89">1、定义</a></li><li><a href="#2%E8%AE%A1%E7%AE%97%E6%8E%A8%E5%AF%BC">2、计算推导</a></li><li><a href="#3%E5%BA%94%E7%94%A8">3、应用</a><ul><li><a href="#1%E7%AE%80%E5%8D%95%E5%B9%B3%E9%9D%A2%E7%9A%84%E8%BD%AC%E6%8D%A2">1、简单平面的转换</a></li><li><a href="#2%E5%9C%A8%E5%9B%9B%E8%BD%B4%E4%B8%AD%E6%B1%82%E5%8F%962d%E7%82%B9%E5%88%B03d%E7%82%B9%E7%9A%84%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB">2、在四轴中求取2D点到3D点的转换关系</a><ul><li><a href="#21-%E7%9B%B8%E6%9C%BA%E5%9C%A8%E6%89%8B%E4%B8%8A">2.1 相机在手上</a><ul><li><a href="#211-%E8%BD%AC%E6%8D%A2%E6%96%B9%E7%A8%8B">2.1.1 转换方程</a></li><li><a href="#212-%E7%9B%B4%E6%8E%A5%E6%B3%95">2.1.2 直接法</a></li><li><a href="#213-%E6%97%8B%E8%BD%AC%E6%B3%95">2.1.3 旋转法</a></li></ul></li><li><a href="#22-%E7%9B%B8%E6%9C%BA%E4%B8%8D%E5%9C%A8%E6%89%8B%E4%B8%8A">2.2 相机不在手上</a><ul><li><a href="#221-%E8%BD%AC%E6%8D%A2%E6%96%B9%E7%A8%8B">2.2.1 转换方程</a></li><li><a href="#222-%E7%9B%B4%E6%8E%A5%E6%B3%95">2.2.2 直接法</a></li><li><a href="#223-%E5%9C%86%E5%BC%A7%E6%B3%95">2.2.3 圆弧法</a></li></ul></li><li><a href="#23-%E7%9B%B4%E6%8E%A5%E5%88%A9%E7%94%A8axxb%E8%A7%A3%E5%86%B3">2.3 直接利用AXXB解决</a></li><li><a href="#%E6%80%BB%E7%BB%93%E6%A2%B3%E7%90%86">总结梳理</a></li></ul></li></ul></li></ul></li><li><a href="#%E4%B8%89%E5%8D%95%E7%9B%AE%E6%A0%87%E5%AE%9A">三、单目标定</a><ul><li><a href="#1%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%A0%87%E5%AE%9A%E7%90%86%E6%83%B3%E6%83%85%E5%86%B5%E4%B8%8B%E6%97%A0%E7%95%B8%E5%8F%98">1、如何进行标定？（理想情况下无畸变）</a></li><li><a href="#2%E5%AE%9E%E9%99%85%E6%83%85%E5%86%B5%E4%B8%8B%E6%9C%89%E7%95%B8%E5%8F%98">2、实际情况下（有畸变）</a><ul><li><a href="#21%E5%BE%84%E5%90%91%E7%95%B8%E5%8F%98">2.1、径向畸变</a></li><li><a href="#22%E5%88%87%E5%90%91%E7%95%B8%E5%8F%98">2.2、切向畸变</a></li><li><a href="#23%E6%80%BB%E7%BB%93">2.3、总结</a></li></ul></li><li><a href="#3%E7%96%91%E9%97%AE">3、疑问</a><ul><li><a href="#31%E6%A0%87%E5%AE%9A%E7%9A%84%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E4%BB%8E%E4%BD%95%E8%80%8C%E6%9D%A5">3.1、标定的世界坐标从何而来</a></li><li><a href="#32%E4%B8%80%E5%BC%A0%E5%9B%BE%E5%83%8F%E4%B9%9F%E8%83%BD%E5%BE%97%E5%88%B0%E6%A0%87%E5%AE%9A%E7%BB%93%E6%9E%9C">3.2、一张图像也能得到标定结果</a></li></ul></li><li><a href="#4%E8%87%AA%E5%8A%A8%E6%A0%87%E5%AE%9A">4、自动标定</a><ul><li><a href="#eye-in-hand">eye in hand</a></li><li><a href="#eye-to-hand">eye to hand</a></li></ul></li></ul></li><li><a href="#%E5%9B%9B%E5%8F%8C%E7%9B%AE%E6%A0%87%E5%AE%9A">四、双目标定</a><ul><li><a href="#1%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">1、基本概念</a><ul><li><a href="#10%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%95">1.0、对极几何</a></li><li><a href="#11%E6%9C%AC%E5%BE%81%E7%9F%A9%E9%98%B5essential-matrix">1.1、本征矩阵(Essential matrix)</a></li><li><a href="#12%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5fundamental-matrix">1.2、基础矩阵(Fundamental matrix)</a></li><li><a href="#13%E7%9F%A9%E9%98%B5q">1.3、矩阵Q</a></li><li><a href="#14bouguet%E6%9E%81%E7%BA%BF%E7%9F%AB%E6%AD%A3">1.4、bouguet极线矫正</a></li></ul></li><li><a href="#2%E8%AE%A1%E7%AE%97">2、计算</a></li></ul></li><li><a href="#%E4%BA%94%E7%9C%BC%E6%89%8B%E6%A0%87%E5%AE%9A">五、眼手标定</a><ul><li><a href="#1%E7%9B%B8%E6%9C%BA%E5%AE%89%E8%A3%85%E5%9C%A8%E6%9C%BA%E6%A2%B0%E6%89%8B%E4%B8%8A">1、相机安装在机械手上</a></li><li><a href="#2%E7%9B%B8%E6%9C%BA%E4%B8%8D%E5%AE%89%E8%A3%85%E5%9C%A8%E6%9C%BA%E6%A2%B0%E6%89%8B%E4%B8%8A">2、相机不安装在机械手上</a></li><li><a href="#3%E5%9B%9B%E8%BD%B4tz%E7%9A%84%E8%AE%A1%E7%AE%97">3、四轴tz的计算</a></li><li><a href="#4axxb">4、AXXB</a></li></ul></li></ul><!-- /TOC --><h2 id="一、机器视觉几何坐标概论"><a href="#一、机器视觉几何坐标概论" class="headerlink" title="一、机器视觉几何坐标概论"></a>一、机器视觉几何坐标概论</h2><p>机器视觉系统有三大坐标系，分别是：1、世界坐标系，2、摄像机坐标系，3、图像（像素）坐标系；</p><h3 id="1、世界坐标系"><a href="#1、世界坐标系" class="headerlink" title="1、世界坐标系"></a>1、世界坐标系</h3><p>世界坐标系（Xw，Yw，Zw）是目标物体位置的参考系，根据运算方便自由设置圆点位置，可以位于机器手底座或者机器手前端执行器上。</p><p>其主要作用有</p><p>(1)盛放物体的三维坐标；</p><p>(2)标定的时候根据原点确定标定物的位置；</p><p>(3)给定出两个摄像机相对于世界坐标系的位置，从而求出两个或多个相机之间的坐标关系；</p><h3 id="2、摄像机坐标系"><a href="#2、摄像机坐标系" class="headerlink" title="2、摄像机坐标系"></a>2、摄像机坐标系</h3><p>摄像机坐标系（Xc，Yc，Zc）是摄像机在自己角度上的坐标系，原点在摄像机的光心上，Z轴与摄像机光轴平行，即摄像机的镜头拍摄方向。</p><h3 id="3、图像（像素）坐标系"><a href="#3、图像（像素）坐标系" class="headerlink" title="3、图像（像素）坐标系"></a>3、图像（像素）坐标系</h3><h4 id="3-1、图像坐标系"><a href="#3-1、图像坐标系" class="headerlink" title="3.1、图像坐标系"></a>3.1、图像坐标系</h4><p>图像坐标系（x，y）单位米或毫米，是连续图像坐标或者空间坐标，以图片对角线交点作为基准原点建立的坐标系。</p><h4 id="3-2、像素坐标系"><a href="#3-2、像素坐标系" class="headerlink" title="3.2、像素坐标系"></a>3.2、像素坐标系</h4><p>像素坐标系（u，v）单位尺度为一个pixel，是离散图像坐标或像素坐标，原点在图片的左上角。</p><h3 id="4、坐标系之间的关系"><a href="#4、坐标系之间的关系" class="headerlink" title="4、坐标系之间的关系"></a>4、坐标系之间的<strong>关系</strong></h3><p>当我们在图片中确定了某个物体的位置，如何让机器手去到空间中的实际位置进行抓取呢？这就需要对坐标进行转换。而从像素点到空间点的转换与空间点到像素点的转换是相反的，我们先将后者的推导过程。</p><h4 id="4-1、图像坐标系与像素坐标系"><a href="#4-1、图像坐标系与像素坐标系" class="headerlink" title="4.1、图像坐标系与像素坐标系"></a>4.1、图像坐标系与像素坐标系</h4><p>图像坐标系与像素坐标系的关系为：</p><p>$$<br>f(x)&#x3D;<br>\begin{cases}<br>u &#x3D; \frac{x}{dx} + u0 \<br>v &#x3D; \frac{y}{dy} + v0<br>\end{cases}<br>$$</p><p>dx代表一个像素的宽度（x方向），与x同单位，x&#x2F;dx表示x轴上有多少个像素，同理y&#x2F;dy表示y轴上的像素个数，(u0，v0)是图像平面中心。</p><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/fixture.png" width="300" align=center /><p>将上述关系转换为矩阵形式：</p><p>$$<br>\left[<br> \begin{matrix}<br>   u\<br>   v\<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   \frac{1}{dx} &amp; 0 &amp; u0\<br>   0 &amp; \frac{1}{dy} &amp; v0\<br>   0 &amp; 0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x\<br>   y\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><h4 id="4-2、相机坐标系与图像坐标系"><a href="#4-2、相机坐标系与图像坐标系" class="headerlink" title="4.2、相机坐标系与图像坐标系"></a>4.2、相机坐标系与图像坐标系</h4><p>从相机坐标系到图像坐标系是一个三维坐标到二维坐标（3D-&gt;2D）的过程，称之为透视投影变换。为了求解它们之间的关系，将普通图像坐标（x，y）拓展为齐次坐标（x，y，1）。空间中的某点，投影到图像平面上的点与相机的光心在一条直线上。以光心为原点建立相机坐标系：</p><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/fctofi.png" width="300" align=center /><p>根据相似三角形关系可以得到以下：</p><p>△ABO_c ~ △oCO_c</p><p>△PBO_c ~ △pCO_c</p><p>$$<br>\frac{AB}{oC} &#x3D; \frac{AO_c}{oO_c} &#x3D; \frac{PB}{pC} &#x3D; \frac{X_c}{x} &#x3D; \frac{Z_c}{f} &#x3D;\frac{Y_c}{y}<br>$$</p><p>$$<br>x &#x3D; f \cdot \frac{X_c}{Z_c},<br>y &#x3D; f \cdot \frac{Y_c}{Z_c}<br>$$</p><p>f为相机焦距（相机光心到成像平面的距离）</p><p>用矩阵形式表示为：</p><p>$$<br>Zc<br>\left[<br> \begin{matrix}<br>   x\<br>   y \<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   f &amp; 0 &amp; 0 &amp; 0\<br>   0 &amp; f &amp; 0 &amp; 0\<br>   0 &amp; 0 &amp; 1 &amp; 0<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   Xc\<br>   Yc\<br>   Zc\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>统一将成像平面上的点用(u,v)表示：</p><p>$$<br>Z<br>\left[<br> \begin{matrix}<br>   u \<br>   v \<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   f &amp; 0 &amp; 0 &amp; 0\<br>   0 &amp; f &amp; 0 &amp; 0\<br>   0 &amp; 0 &amp; 1 &amp; 0<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   Xc\<br>   Yc\<br>   Z\<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   f \cdot Xc\<br>   f \cdot Yc\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>得图像点与空间点的关系为：</p><p>$$<br>u &#x3D;\frac {f \cdot X_c}{Z},<br>v &#x3D;\frac {f \cdot Y_c}{Z}<br>$$</p><h4 id="4-3、世界坐标系与相机坐标系"><a href="#4-3、世界坐标系与相机坐标系" class="headerlink" title="4.3、世界坐标系与相机坐标系"></a>4.3、世界坐标系与相机坐标系</h4><p>世界坐标（Xw，Yw，Zw）与相机坐标（Xc，Yc，Zc）同为三维坐标（右手系，三轴互相垂直），两个坐标系的关系为刚体变换（刚体变换：当物体不发生形变时，对一个几何物体作旋转，平移的运动）。可以先凭空想象下，有两个坐标系A与B，如何将A坐标系下的坐标转换到B坐标系表示，首先将A坐标系以原点为基准任意旋转，使其x轴，y轴，z轴与B坐标轴平行且同方向，接着平移AB坐标系原点的直线距离，就可以将A坐标系下的坐标转换到B坐标系，这个旋转Rotation与平移Transport就是需要求得的两个三维坐标之间的关系。</p><p>用以下等式表示两个坐标系之间的关系：</p><p>$$<br>\left[<br> \begin{matrix}<br>   Xc\<br>   Yc \<br>   Zc<br>  \end{matrix}<br>\right]&#x3D;<br>R<br>\left[<br> \begin{matrix}<br>   Xw\<br>   Yw\<br>   Zw<br>  \end{matrix}<br>\right]<br>+T<br>$$</p><p>其中旋转矩阵R可以看成空间坐标分别沿着X，Y，Z轴的三个旋转矩阵点乘得到的结果。</p><p>当绕Z轴旋转$\theta$角度，新旧坐标的关系为：</p><p>$$<br>\begin{cases}<br>x &#x3D; x’cos\theta - y’sin\theta \<br>y &#x3D; x’sin\theta + y’cos\theta \<br>z &#x3D; z’<br>\end{cases}<br>$$</p><p>用矩阵表示为：</p><p>$$<br>\left[<br> \begin{matrix}<br>   x\<br>   y \<br>   z<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   cos\theta &amp; -sin\theta &amp; 0\<br>   sin\theta &amp; cos\theta &amp; 0\<br>   0 &amp; 0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’\<br>   z’<br>  \end{matrix}<br>\right]&#x3D;<br>R1<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’ \<br>   z’<br>  \end{matrix}<br>\right]<br>$$</p><p>同理，绕X轴，Y轴旋转$\delta$和$\omega$角度，可以得到：</p><p>$$<br>\left[<br> \begin{matrix}<br>   x\<br>   y \<br>   z<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   1 &amp; 0 &amp; 0\<br>   0 &amp; cos\delta &amp; sin\delta<br>\<br>   0 &amp; -sin\delta<br> &amp; cos\delta<br>\<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’\<br>   z’<br>  \end{matrix}<br>\right]&#x3D;<br>R2<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’ \<br>   z’<br>  \end{matrix}<br>\right]<br>$$</p><p>$$<br>\left[<br> \begin{matrix}<br>   x\<br>   y\<br>   z<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   cos\omega &amp; 0 &amp; -sin\omega\<br>   0 &amp; 1 &amp; 0\<br>   sin\omega &amp; 0 &amp; cos\omega<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’\<br>   z’<br>  \end{matrix}<br>\right]&#x3D;<br>R3<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’ \<br>   z’<br>  \end{matrix}<br>\right]<br>$$</p><p>于是，得到旋转矩阵R &#x3D; R1*R2*R3，维度为3X3，T为平移矩阵，维度为3X1。</p><p>拓展为其次坐标：</p><p>$$<br>\left[<br> \begin{matrix}<br>   Xc\<br>   Yc \<br>   Zc \<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   R &amp; T\<br>   0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   Xw\<br>   Yw\<br>   Zw\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><h4 id="4-4、从世界坐标到像素坐标"><a href="#4-4、从世界坐标到像素坐标" class="headerlink" title="4.4、从世界坐标到像素坐标"></a>4.4、从世界坐标到像素坐标</h4><p>综合上面推导的过程，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[世界坐标] --&gt; B[相机坐标]</span><br><span class="line">B --&gt; C[理想图像坐标]</span><br><span class="line">C -- 不考虑畸变--&gt; D[像素坐标]</span><br></pre></td></tr></table></figure><p>以上顺序用矩阵表示为不断左乘下一步，即：</p><p>$$<br>Zc<br>\left[<br> \begin{matrix}<br>   u\<br>   v \<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   \frac{1}{dx} &amp; 0 &amp; u0\<br>   0 &amp; \frac{1}{dy} &amp; v0\<br>   0 &amp; 0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   f &amp; 0 &amp; 0 &amp; 0\<br>   0 &amp; f &amp; 0 &amp; 0\<br>   0 &amp; 0 &amp; 1 &amp; 0<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   R &amp; T\<br>   0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   Xw\<br>   Yw\<br>   Zw\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>等式右边的前两个矩阵称的乘积为相机内参，第三个矩阵称为相机外参(Toc)，后面的单目相机标定，就是为了求解相机的内外参数。</p><p>至此，机器视觉几何坐标概论记录完了，接下来会陆续记录我所参与的项目中包含标定的内容。</p><h2 id="二、平面标定（Homography变换）"><a href="#二、平面标定（Homography变换）" class="headerlink" title="二、平面标定（Homography变换）"></a>二、平面标定（Homography变换）</h2><h3 id="1、定义"><a href="#1、定义" class="headerlink" title="1、定义"></a>1、定义</h3><p>单应性(homography)变换用来描述物体在两个平面之间的转换关系，是对应齐次坐标下的线性变换，可以通过矩阵表示：</p><p>$$<br>X’ &#x3D; H·X<br>$$</p><h3 id="2、计算推导"><a href="#2、计算推导" class="headerlink" title="2、计算推导"></a>2、计算推导</h3><p>带入数据（x,y）为图片上的点位置：</p><p>$$<br>\left[<br> \begin{matrix}<br>   x’\<br>   y’\<br>   w<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   h1 &amp; h2 &amp; h3\<br>   h4 &amp; h5 &amp; h6\<br>   h7 &amp; h8 &amp; h9<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x\<br>   y\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>因为是齐次坐标系，方程左右同时除h9</p><p>$$<br>\left[<br> \begin{matrix}<br>   x1’\<br>   y1’\<br>   w’<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   \frac{x’}{h9}\<br>   \frac{y’}{h9}\<br>   \frac{z’}{h9}<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   h1’ &amp; h2’ &amp; h3’\<br>   h4’ &amp; h5’ &amp; h6’\<br>   h7’ &amp; h8’ &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   x\<br>   y\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>将矩阵展开得到：</p><p>公式①：<br>$$<br>\begin{cases}<br>x1’ &#x3D; h1’x+h2’y+h3’ \<br>y1’ &#x3D; h4’x+h5’y+h6’ \<br>w’ &#x3D; h7’x+h8’y+h9’<br>\end{cases}<br>$$<br>将下面的矩阵用已知的观测值代替：</p><p>$$<br>\left[<br> \begin{matrix}<br>   x1’\<br>   y1’\<br>   w’<br>  \end{matrix}<br>\right]<br>&#x3D;&#x3D;&gt;(x^R, y^R)<br>$$</p><p>根据齐次坐标的齐次性质：</p><p>公式②：<br>$$<br>\frac{x1’}{w’}&#x3D;x^R,\frac{y1’}{w’}&#x3D;y^R<br>$$</p><p>结合公式①②：</p><p>公式③：<br>$$<br>\begin{cases}<br>x^R &#x3D;  \frac{h1’x+h2’y+h3’}{h7’+h8’+1}\<br>y^R &#x3D;  \frac{h4’x+h5’y+h6’}{h7’+h8’+1}<br>\end{cases}<br>$$<br>在公式③中</p><p>$$<br>(x, y)，(x^R, y^R)<br>$$<br>为观测到的已知参数，未知参数有h1’<del>h8’共8个，一对点能够产生两个方程，则求解公式③至少需要四组点对，才能求出h1’</del>h8’。一般的数据会多于4组点对，用最小二乘法或ransac来获取最优参数。</p><p>求解过后，h1’~h8’为已知，</p><p>$$<br>H’&#x3D;<br>\left[<br> \begin{matrix}<br>   h1’ &amp; h2’ &amp; h3’\<br>   h4’ &amp; h5’ &amp; h6’\<br>   h7’ &amp; h8’ &amp; 1<br>  \end{matrix}<br>\right]<br>$$<br>可用于单应性变换的计算。</p><h3 id="3、应用"><a href="#3、应用" class="headerlink" title="3、应用"></a>3、应用</h3><h4 id="1、简单平面的转换"><a href="#1、简单平面的转换" class="headerlink" title="1、简单平面的转换"></a>1、简单平面的转换</h4><p>图片A中的点P为(x,y)，求对应在另外一个视角的图片B点P’(x’,y’)？</p><p>$$<br>\left[<br> \begin{matrix}<br>   x2\<br>   y2\<br>   z2<br>  \end{matrix}<br>\right]&#x3D;<br>H’<br>\left[<br> \begin{matrix}<br>   x\<br>   y\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>$$<br>P’(x’,y’) &#x3D; (\frac{x2}{z2},\frac{y2}{z2})<br>$$</p><p>——————————————————–2020更新 华丽的分割线——————————————————–</p><h4 id="2、在四轴中求取2D点到3D点的转换关系"><a href="#2、在四轴中求取2D点到3D点的转换关系" class="headerlink" title="2、在四轴中求取2D点到3D点的转换关系"></a>2、在四轴中求取2D点到3D点的转换关系</h4><h5 id="2-1-相机在手上"><a href="#2-1-相机在手上" class="headerlink" title="2.1 相机在手上"></a>2.1 相机在手上</h5><p>求取</p><p>$$<br>{^r}H{_c}<br>$$</p><h6 id="2-1-1-转换方程"><a href="#2-1-1-转换方程" class="headerlink" title="2.1.1 转换方程"></a>2.1.1 转换方程</h6><p>$$<br>{^rP_o &#x3D;  ^rT_t \cdot ^tH_C \cdot ^CP_o}<br>$$</p><p>参数解释：</p><p>$^rP_o$(4×1)：object在Robot坐标系下的表示</p><p>$^rT_t$(4×4)：Tool到Robot的转换矩阵，即机械手示教器上读回的数值</p><p>$^tH_C$(3×3)：图像平面h1到Tool平面转换矩阵</p><p>$^CP_o$(2×1&#x3D;&#x3D;齐次变化&#x3D;&#x3D;&gt;3×1)：图像中的点</p><h6 id="2-1-2-直接法"><a href="#2-1-2-直接法" class="headerlink" title="2.1.2 直接法"></a>2.1.2 直接法</h6><p>计算：delta pos 与图像点平面的关系</p><p>$$<br>^tT_r \cdot ^{ri}P_o &#x3D;  ^tH_C \cdot ^{Ci}P_o<br>$$</p><p>参数解释：$^tT_r$保持不变，$^tH_C$为所求的参数，$^{Ci}P_o$与$^{ri}P_o$一一对应。</p><p>适用条件：u轴不变的情况，末端执行器与机械手只存在x、y的偏移量。</p><p>(提示：机械手位姿数目&#x3D;&#x3D;图像数目+1)</p><p>该方法得到的H矩阵是图像点与△Pos (△Pos &#x3D; $P_i - P_{origin}$) 之间的关系，所以在后期的使用中，需要先确定一个$P_{origin}$，再利用图像与Homo关系得到△Pos，再叠加上$P_{origin}$，得到新的一个$^rP_i$。</p><p>操作过程：</p><div align="center"><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/plane_calibration_4axis_dir_flow.png" /></div><p>为什么直接法所需要的数据，机械手poses会比图像坐标多一个呢？<br>解答：根据操作流程，先选择一个能够看到标定板的位置，作为基准点。<br>采集数据过程：手动更换标定板位置依旧在视野中$img_i$，控制机械手末端到标定板上点$pose_i$，循环往复多次采集。</p><h6 id="2-1-3-旋转法"><a href="#2-1-3-旋转法" class="headerlink" title="2.1.3 旋转法"></a>2.1.3 旋转法</h6><p>计算:Pot 与 图像点平面的关系</p><p>$$<br>^{ti}T_r \cdot ^rP_o &#x3D;  ^tH_C \cdot ^{Ci}P_o<br>$$</p><p>参数解释：$^rP_o$为旋转法所求量，保持不变，$^tH_C$为所求的参数，$^{Ci}P_o$与$^{ti}T_r$一一对应。</p><p>操作过程(保持标定板不能动)：</p><div align="center"><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/plane_calibration_4axis_cam_in_hand_rot_flow.webp" /></div><div align="center"><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/plane_calibration_4axis_cam_in_hand_rotation.png" /></div><p>旋转前后分别记下tool的坐标为</p><p>$$<br>^rP_1;<br>^rP_2<br>$$</p><p>则可以求得object在robot下的坐标：</p><p>$$<br>^rP_o &#x3D; (^rP_1+^rP_2)&#x2F;2<br>$$</p><!-- $$^rP{_o}=\frac{^rP_1+^rP_2}{2}$$ --><p>标定过程：在标定板与相机视野内的前提下移动机械手(xy平面，z,u,v,w保持不变)并拍摄图片，存储每个位置与每张图片，采集9组数据，将采集的数据连入PlaneCalibration工具，计算标定结果。</p><p>产生的数据为：旋转法对应的两张图片与两个机械手位姿，九点标定对应的九张图片与九个机械手位姿数据。将图像整合在一个文件夹里，机械手位姿整合在一份文件里，按顺序保存，注意数据的对应关系。</p><p>标定过程只用到了标定板上的某个点作为object，当选定了某个点A，你的旋转法就需要保持点A在图像中位置不变。用标定板的作用是为了在视野中更好地识别点A，当你的目标够明显，也可以不适用标定板(一般不采用)。</p><p>使用如上方法，根据图像中object的点，能得到它在Robot坐标系(xy平面，z,u,v,w保持不变)的位置，就可以将坐标发给机械手执行，该方法不用求取相机的内外参数，不考虑图像的畸变，可以用于对精度要求不高的项目。</p><h5 id="2-2-相机不在手上"><a href="#2-2-相机不在手上" class="headerlink" title="2.2 相机不在手上"></a>2.2 相机不在手上</h5><p>求取相机坐标系与机械手的底座坐标系的变换关系</p><p>$$<br>{^r}H{_c}<br>$$</p><h6 id="2-2-1-转换方程"><a href="#2-2-1-转换方程" class="headerlink" title="2.2.1 转换方程"></a>2.2.1 转换方程</h6><p>$$<br>^rT_t\cdot ^tP_o &#x3D; ^rP_o &#x3D; ^rH_c \cdot ^cP_o<br>$$</p><p>参数解释：</p><p>$^rT_t$(4×4)：Tool到Robot的转换矩阵，即机械手示教器上读回的数值</p><p>${^r}P{_o}$(3×1)：object在robot下的位置</p><p>${^r}H{_c}$(3×3)：图像平面h1到Tool平面转换矩阵</p><p>${^c}P{_t}$(2×1&#x3D;&#x3D;齐次变化&#x3D;&#x3D;&gt;3×1)：tool在图像中的点</p><h6 id="2-2-2-直接法"><a href="#2-2-2-直接法" class="headerlink" title="2.2.2 直接法"></a>2.2.2 直接法</h6><p>$$<br>^rP_o &#x3D; ^rH_c \cdot ^cP_o<br>$$</p><p>标定过程：</p><div align="center"><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/plane_calibration_4axis_dir_flow.png" /></div><h6 id="2-2-3-圆弧法"><a href="#2-2-3-圆弧法" class="headerlink" title="2.2.3 圆弧法"></a>2.2.3 圆弧法</h6><p>$$<br>^rP_o &#x3D; ^rH_c \cdot ^cP_o<br>$$</p><p>圆弧法需要将标定板绑在机械手上，该方法也是为了找到$^rP_o$与$^cP_o$两个数据集间的转换关系。每个Poc都需要由3张相同xy(robot base)坐标下相机拍得的图像，用拟合圆心的方法得到。<strong>object定义为拟合得到的圆心</strong>，即机械手最末端。</p><p>(3张图像拟合的圆心是在本软件中使用的默认图像张数，所以该方法所需要的机械手位姿点数与图像张数必须相等，且是3的倍数。)</p><p>因为只是标定两个平面之间的关系，$^rP_o$的xy与$^rT_t$的xy数据相同。</p><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/plane_calibration_4axis_cam_to_hand_circle.png" style="zoom:100%" /></center><p>用圆弧法需要将标定板绑在机械手上，该方法也是为了找到$^rP_o$与$^cP_o$两个数据集间的转换关系</p><p>操作步骤：</p><div align="center"><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/plane_calibration_4axis_cam_to_hand_circle_flow.png" /></div><h5 id="2-3-直接利用AXXB解决"><a href="#2-3-直接利用AXXB解决" class="headerlink" title="2.3 直接利用AXXB解决"></a>2.3 直接利用AXXB解决</h5><p>eye in hand：</p><p>$$<br>^rT_o&#x3D;^rT_t \cdot ^tT_c \cdot ^cT_o<br>$$</p><p>列两个方程：<br>$$<br>^rT_o&#x3D;^rT_t^{1} \cdot ^tT_c \cdot ^cT_o^{1}<br>$$<br>$$<br>^rT_o&#x3D;^rT_t^{2} \cdot ^tT_c \cdot ^cT_o^{2}<br>$$</p><p>列出方程：<br>$$<br>^rT_t^{1} \cdot ^tT_c \cdot ^cT_o^{1}&#x3D;^rT_t^{2} \cdot ^tT_c \cdot ^cT_o^{2}<br>$$</p><p>化简得:<br>$$<br>^tT_r^{1} \cdot ^rT_t^{2} \cdot ^tT_c&#x3D;^tT_c \cdot ^cT_o^{1} \cdot ^oT_c^{2}<br>$$</p><p>$$<br>AX&#x3D;XB<br>$$</p><p>eye to hand:</p><p>标定板绑在手末端</p><p>$$<br>^tT_o&#x3D;^tT_r \cdot ^rT_c \cdot ^cT_o<br>$$</p><p>列两个方程：<br>$$<br>^tT_o&#x3D;^tT_r^{1} \cdot ^rT_c \cdot ^cT_o^{1}<br>$$</p><p>$$<br>^tT_o&#x3D;^tT_r^{2} \cdot ^rT_c \cdot ^cT_o^{2}<br>$$</p><p>$$<br>^tT_r^{1} \cdot ^rT_c \cdot ^cT_o^{1}&#x3D;^tT_r^{2} \cdot ^rT_c \cdot ^cT_o^{2}<br>$$</p><p>化简得:<br>$$<br>^rT_t^{2} \cdot ^tT_r^{1} \cdot ^rT_c &#x3D; ^rT_c \cdot ^cT_o^{2} \cdot ^oT_c^{1}<br>$$</p><p>$$<br>AX&#x3D;XB<br>$$</p><h5 id="总结梳理"><a href="#总结梳理" class="headerlink" title="总结梳理"></a>总结梳理</h5><p>眼在手上，通过图像点Poc与Pot的关系，平面标定出Tct。</p><p>建模版子来,图像点Poc与标定量Tct,得到Pot, Pot与Ttr得到Por，就是现在图像点在Robot下的3d位置，多个图像点计算多个Por</p><p>实际板子来，同上一步骤，计算新的多个Por，与建模的Por计算homo关系，应用到示教点，得到新的电胶点，over～</p><h2 id="三、单目标定"><a href="#三、单目标定" class="headerlink" title="三、单目标定"></a>三、单目标定</h2><p>从之前的<br><a href="#####4.4%E3%80%81%E4%BB%8E%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E5%88%B0%E5%83%8F%E7%B4%A0%E5%9D%90%E6%A0%87">从世界坐标到像素坐标</a>可以得到：</p><p>$$<br>Zc<br>\left[<br> \begin{matrix}<br>   u\<br>   v \<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   \frac{1}{dx} &amp; 0 &amp; u0\<br>   0 &amp; \frac{1}{dy} &amp; v0\<br>   0 &amp; 0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   f &amp; 0 &amp; 0 &amp; 0\<br>   0 &amp; f &amp; 0 &amp; 0\<br>   0 &amp; 0 &amp; 1 &amp; 0<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   R &amp; T\<br>   0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   Xw\<br>   Yw\<br>   Zw\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>相机标定也是利用了上面这个公式，等式右边前两个矩阵乘积为相机内参，第三为相机外参(也成为Toc)。</p><p>各个参数的含义：dx代表一个像素的宽度（x方向），与x同单位，x&#x2F;dx表示x轴上有多少个像素，同理y&#x2F;dy表示y轴上的像素个数，(u0，v0)是图像平面中心。相机模型类似小孔成像模型，自行百度，也可参考前面的坐标转换关系<a href="#####4.2%E3%80%81%E7%9B%B8%E6%9C%BA%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%9D%90%E6%A0%87%E7%B3%BB">相机坐标系与图像坐标系</a>与<a href="#####4.3%E3%80%81%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E7%9B%B8%E6%9C%BA%E5%9D%90%E6%A0%87%E7%B3%BB">世界坐标系与相机坐标系</a>。</p><p>举个实际的例子,相机内参为：</p><p>$$<br>K &#x3D;<br>\left[<br> \begin{matrix}<br>   \frac{f}{dx} &amp; 0 &amp; u0\<br>   0 &amp; \frac{f}{dy} &amp; v0\<br>   0 &amp; 0 &amp; 1<br>  \end{matrix}<br>\right]<br>$$</p><h3 id="1、如何进行标定？（理想情况下无畸变）"><a href="#1、如何进行标定？（理想情况下无畸变）" class="headerlink" title="1、如何进行标定？（理想情况下无畸变）"></a>1、如何进行标定？（理想情况下无畸变）</h3><p>单目标定的目的就是为了得到<strong>dx,dy,f,R,T</strong>这几个参数（暂时不考虑畸变参数）。而最开始接触标定只是按照不同姿势地摆放标定板，前提当然要固定相机位置，为什么要这么做呢。</p><p>设有N个角点，K个棋盘，则可以列出2NK个方程，未知参数有4+6K个，所以从理论上来说，方程组数大于未知数个数就能解出未知数，即2NK&gt;&#x3D;6K+4，即(N-3)K&gt;&#x3D;2，但对于同一个平面来说，4个角点确定了一个与投影平面共面的四边形，在四个方向同时延伸就可以变成任意四边形，所以N的约束即为4，那么就只有K&gt;&#x3D;2这个条件，理论上至少需要两个视场列得到的2×4×2&#x3D;16个方程才能求得4+6×2&#x3D;16个参数。但是考虑噪声和数值稳定性，图片上的角点可以取多后采用最小二乘法、ransac或其他方法取最优参数。一般标定取10幅7×7或7×8角度不同的图片。到此为止，求出了<strong>dx,dy,f,R,T</strong>这几个参数。  </p><p>当然，标定还有另外一种方式，对于eye in hand 这种case，可以固定标定板，相机在机械手上，随着机械手变化位置，去不同的点位拍照，(当硬件结构确定情况下的大批量生产情况采用这种自动标定方式可以节约人力成本，)提前预设好几个位置，让机械手随机在附近进行拍照，完成内外参的标定。</p><h3 id="2、实际情况下（有畸变）"><a href="#2、实际情况下（有畸变）" class="headerlink" title="2、实际情况下（有畸变）"></a>2、实际情况下（有畸变）</h3><p>理想的针孔模型透过小孔的光线少，导致了相机的曝光时间慢，实际中为了加快图像的生成使用透镜，但同时也引入了畸变。所以畸变发生在相机内部，所需要增加的相应计算多了几个畸变参数(会在下面定义),计算方法参照的方程式仍然是从世界坐标系到像素坐标的转换Tpw，常见的畸变有(1)径向畸变，(2)切向畸变。  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph RL</span><br><span class="line">A[世界坐标] --&gt; B[相机坐标]</span><br><span class="line">B --&gt; C[理想图像坐标]</span><br><span class="line">C -- 不考虑畸变--&gt; D[像素坐标]</span><br><span class="line">C -- 畸变 --&gt; E[实际图像坐标]</span><br><span class="line">E --&gt; D</span><br></pre></td></tr></table></figure><p>如果你用opencv的接口，会有最多(k1,k2,p1,p2[,k3[,k4,k5,k6[,s1,s2,s3,s4[,τx,τy]]]])这么多个畸变参数。</p><h4 id="2-1、径向畸变"><a href="#2-1、径向畸变" class="headerlink" title="2.1、径向畸变"></a>2.1、径向畸变</h4><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/radialdistortionmodel.jpeg"/></center><p>径向畸变的现象有桶形失真与枕形失真。在成像仪光轴中心的畸变为0，沿着镜头半径方向向边缘移动，径向位移增大，畸变越来越严重。畸变的数学模型用参数<strong>k1,k2,k3,k4,k5,k6</strong>（或更多）描述，用主点的泰勒级数展开式前三项（或更多）表示：</p><p>$$<br>\begin{cases}<br>x_{correct} &#x3D;  x(1+k_1r^2+k_2r^4+k_3r^6+…)\<br>y_{correct} &#x3D;  y(1+k_1r^2+k_2r^4+k_3r^6+…)<br>\end{cases}<br>$$</p><h4 id="2-2、切向畸变"><a href="#2-2、切向畸变" class="headerlink" title="2.2、切向畸变"></a>2.2、切向畸变</h4><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/tangentialdistortionmodel.jpeg"/></center><p>切向畸变是由于透镜的安装偏差产生，导致透镜本身与相机成像平面不平行。切向畸变模型用参数<strong>p1,p2</strong>描述：</p><p>$$<br>\begin{cases}<br>x_{correct} &#x3D;  x+[2p_1xy+p_2(r^2+2x^2)]\<br>y_{correct} &#x3D;  y+[2p_2xy+p_1(r^2+2y^2)]<br>\end{cases}<br>$$<br>(r为该点距离成像中心距离)</p><h4 id="2-3、总结"><a href="#2-3、总结" class="headerlink" title="2.3、总结"></a>2.3、总结</h4><p>综合以上两种畸变，得到畸变矫正前后的坐标对应（理想图像坐标到实际图像坐标）关系：</p><p>$$<br>\left[<br> \begin{matrix}<br>   x_c\<br>   y_c<br>  \end{matrix}<br>\right]&#x3D;<br>(1+k_1r^2+k_2r^4+k_3r^6)<br>\left[<br> \begin{matrix}<br>   x_p\<br>   y_p<br>  \end{matrix}<br>\right]<br>+<br>\left[<br> \begin{matrix}<br>   2p_1xy+p_2(r^2+2x^2)\<br>   2p_2xy+p_1(r^2+2y^2)<br>  \end{matrix}<br>\right]<br>$$</p><p>此时，世界坐标系到像素坐标系的关系就多了一个畸变转换：</p><p>$$<br>Zc<br>\left[<br> \begin{matrix}<br>   u\<br>   v \<br>   1<br>  \end{matrix}<br>\right]&#x3D;<br>\left[<br> \begin{matrix}<br>   \frac{1}{dx} &amp; 0 &amp; u0\<br>   0 &amp; \frac{1}{dy} &amp; v0\<br>   0 &amp; 0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   distor transform<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   f &amp; 0 &amp; 0 &amp; 0\<br>   0 &amp; f &amp; 0 &amp; 0\<br>   0 &amp; 0 &amp; 1 &amp; 0<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   R &amp; T\<br>   0 &amp; 1<br>  \end{matrix}<br>\right]<br>\left[<br> \begin{matrix}<br>   Xw\<br>   Yw\<br>   Zw\<br>   1<br>  \end{matrix}<br>\right]<br>$$</p><p>畸变转换至少包含五个参数<strong>k1,k2,k3,p1,p2</strong>,标定需要求得的未知数就有4+5+6K个，所以，为了标定结果的准确性，前面提到的标定取10幅7×7或7×8角度不同的图片是很有必要的。</p><h3 id="3、疑问"><a href="#3、疑问" class="headerlink" title="3、疑问"></a>3、疑问</h3><h4 id="3-1、标定的世界坐标从何而来"><a href="#3-1、标定的世界坐标从何而来" class="headerlink" title="3.1、标定的世界坐标从何而来"></a>3.1、标定的世界坐标从何而来</h4><p>给予标定板的建立世界坐标的xy轴，z轴垂直标定板射出，原点位置定义为标定板的圆孔的第一个点。</p><h4 id="3-2、一张图像也能得到标定结果"><a href="#3-2、一张图像也能得到标定结果" class="headerlink" title="3.2、一张图像也能得到标定结果"></a>3.2、一张图像也能得到标定结果</h4><p>N的约束4还要保留吗？</p><p>一张图片确实可以得到标定结果，会有很多解。</p><h3 id="4、自动标定"><a href="#4、自动标定" class="headerlink" title="4、自动标定"></a>4、自动标定</h3><p>由于标定过程比较繁杂，无非就是尽可能多的在工作平面附近拍摄标定板的图片进行计算。使用机械手自动走点位进行拍照的方法较为节省人力成本。(球面)</p><p>本次分析参考：<a href="https://blog.csdn.net/qq_34193345/article/details/104631302">自动标定分析</a></p><h4 id="eye-in-hand"><a href="#eye-in-hand" class="headerlink" title="eye in hand"></a>eye in hand</h4><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/eye_in_hand.png"/></center><p>原始位置:<br>$$<br>^rT_o &#x3D; ^rT_t \cdot ^tT_c \cdot ^cT_o<br>$$<br>新位置:<br>$$<br>^rT_o &#x3D; ^rT_t^{‘} \cdot ^tT_c \cdot ^cT_o^{‘}<br>$$</p><p>$$<br>^cT_o^{‘} &#x3D; ^cT_o \cdot T<br>$$</p><p>结合以上两个公式，移项化简得：<br>$$<br>^rT_t^{‘} &#x3D; ^rT_o \cdot  T^{-1} \cdot ^oT_t<br>$$</p><h4 id="eye-to-hand"><a href="#eye-to-hand" class="headerlink" title="eye to hand"></a>eye to hand</h4><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/eye_to_hand.png"/></center><p>与eye in hand同理<br>原始位置:<br>$$<br>^rT_c &#x3D; ^rT_t \cdot ^tT_o \cdot ^oT_c<br>$$<br>新位置:<br>$$<br>^rT_c &#x3D; ^rT_t^{‘} \cdot ^tT_o \cdot ^oT_c^{‘}<br>$$</p><p>$$<br>^oT_c^{‘} &#x3D; T^{-1} \cdot ^oT_c<br>$$</p><p>结合以上两个公式，移项化简得：<br>$$<br>^rT_t^{‘} &#x3D; ^rT_o \cdot  T \cdot ^oT_t<br>$$</p><p>以上的T均假设移动的object的偏移量。</p><h2 id="四、双目标定"><a href="#四、双目标定" class="headerlink" title="四、双目标定"></a>四、双目标定</h2><p>为了得到图片中物体的深度信息，引入双目视觉。</p><p>双目标定在<a href="##%E4%B8%89%E3%80%81%E5%8D%95%E7%9B%AE%E6%A0%87%E5%AE%9A">单目标定</a>的基础上，标定出左右摄像机坐标系之间的相对关系。</p><h3 id="1、基本概念"><a href="#1、基本概念" class="headerlink" title="1、基本概念"></a>1、基本概念</h3><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/stereocam.png"/></center><p>所涉及的专业名词有对极几何，本征矩阵，基础矩阵。</p><p>双目相机基于对极几何的理论，本征矩阵E(essential_matrix)包含在物理空间两个摄像机相关的旋转与平移信息，基础矩阵F(fundamental_matrix)除了包含E的信息还包括两个相机的内参数。E是几何意义上的，与成像仪无关，将左相机观测到的点P的坐标与右相机观测到的相同点的坐标关联起来(<strong>相机坐标系下</strong>)，F则是将左相机的像平面上的点与右相机观测到的像平面上相同点关联起来(<strong>像素坐标系</strong>)。</p><h4 id="1-0、对极几何"><a href="#1-0、对极几何" class="headerlink" title="1.0、对极几何"></a>1.0、对极几何</h4><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/EpipolarGeometry.png"/></center><p>立体成像系统的几何基础被称作“对极几何”</p><p>投影点：XL，XR</p><p>极点：相机光心投影到另一个相机的像平面上，eL，eR。</p><p>基线:两摄像机光心之间的距离</p><p>极线：直线Ol-XL因为与左相机的光学中心对其，所以在左相机被视为一个点（直线上的任意一点都投影到像平面上的XL点），但是，右侧相机会将此视为图像平面的一条线（图中红线），这条在右侧相机像平面上的直线eR-XR称为极线，同理，左相机像平面上的直线XL-eL也称为极线。</p><p>极线平面：XL-eL-eR-XR构成的平面</p><p>主点：主光线和相平面的相交位置OL，OR，两对极点与主点在同一3D直线上。</p><p>摄像机视图中的每一个三维点都被包含在与每个图像相交的极面中，二者相交产生的直线是极线。<br>给定一幅图像的特征，在另一图像的匹配视图一定位于对应极线。这称为“极线约束” 。<br>极限约束意味着，一旦我们知道立体实验设备之间的对极几何，在两幅图像中匹配的二维搜索可以转变为沿着极线的一维搜索。</p><h4 id="1-1、本征矩阵-Essential-matrix"><a href="#1-1、本征矩阵-Essential-matrix" class="headerlink" title="1.1、本征矩阵(Essential matrix)"></a>1.1、本征矩阵(Essential matrix)</h4><p>本征矩阵用字母E来表示，物理意义是左右相机坐标系相互转换的矩阵,表示几何意义，与单成像仪无关，用来描述左右相机图像平面上对应点<strong>在各自相机坐标系</strong>之间的关系。</p><p>特性：<br>1）本质矩阵是由对极约束定义的。由于对极约束是等式为0的约束，所以对E乘以任何非零常数后，对极约束依然满足。这一点被称为E在不同尺度下是等价的。<br>2）根据E&#x3D;t∧R，可以证明，本质矩阵E的奇异值必定式[σ,σ,0]T。这称为本质矩阵的内在性质。可以理解为：一个3×3的矩阵是本征矩阵的充要条件是对它奇异值分解后，它有两个相等的奇异值，并且第三个奇异值为0。<br>3）由于平移和旋转各有3个自由度，所以t∧R共有6个自由度。但由于尺度等价性，故E实际上只有5个自由度。</p><h4 id="1-2、基础矩阵-Fundamental-matrix"><a href="#1-2、基础矩阵-Fundamental-matrix" class="headerlink" title="1.2、基础矩阵(Fundamental matrix)"></a>1.2、基础矩阵(Fundamental matrix)</h4><p>给本征矩阵E增加相机内参矩阵M的相关信息，就能得到描述同一个物理点在左右相机图像平面上对应点<strong>在各自像素坐标系</strong>下的关系。</p><p>参考: <a href="https://docs.opencv.org/4.0.0/d9/d0c/group__calib3d.html#gae420abc34eaa03d0c6a67359609d8429">opencv官方文档</a></p><p>极线模型(两像素点关系)：</p><p>$$<br>[P_2:1]^T \cdot F \cdot[P_1:1] &#x3D; 0<br>$$</p><h4 id="1-3、矩阵Q"><a href="#1-3、矩阵Q" class="headerlink" title="1.3、矩阵Q"></a>1.3、矩阵Q</h4><p>矩阵Q实现了视差图$disparity(x,y)$到3D世界坐标$(X,Y,Z,W)$</p><h1 id="left-begin-matrix-X-amp-Y-amp-Z-amp-W-end-matrix-right-T"><a href="#left-begin-matrix-X-amp-Y-amp-Z-amp-W-end-matrix-right-T" class="headerlink" title="$$\left[ \begin{matrix}   X &amp; Y &amp; Z &amp;W  \end{matrix}\right]^T"></a>$$<br>\left[<br> \begin{matrix}<br>   X &amp; Y &amp; Z &amp;W<br>  \end{matrix}<br>\right]^T</h1><p>Q*<br>\left[<br> \begin{matrix}<br>   x &amp; y &amp; disparity(x,y) &amp; 1<br>  \end{matrix}<br>\right]^T<br>$$</p><p>$$<br>\left[<br> \begin{matrix}<br>   1 &amp; 0 &amp; 0 &amp; -c_x\<br>   0 &amp; 1 &amp; 0 &amp; -c_y\<br>   0 &amp; 0 &amp; 0 &amp; f\<br>   0 &amp; 0 &amp; -1&#x2F;T_x &amp; (c_x-c_x^{‘})&#x2F;T_x<br>  \end{matrix}<br>\right]<br>$$</p><p>$(c_x, c_y)$左相机主点坐标，$(c_x^{‘}, c_y^{‘})$右相机主点坐标，双目矫正后$c_y&#x3D;&#x3D;c_y^{‘}$成立，$f$焦距，$T_x$基线长，即两相机光心之间的距离。</p><h4 id="1-4、bouguet极线矫正"><a href="#1-4、bouguet极线矫正" class="headerlink" title="1.4、bouguet极线矫正"></a>1.4、bouguet极线矫正</h4><p>为了进行深度计算与三维重建，应该在双目标定之后先进行极线矫正。</p><p>校正前两个相机的光轴是一个倒V字形，如上对极几何图形所示，左右相机像平面不平行，极点在像平面上；</p><p>相比，校正后两相机光轴平行，像点在左右图像上的高度一致(极线矫正目的)，是为了做立体匹配时，只需要在同一行上搜索左右像平面的匹配点，提升效率。</p><p>矫正方法：将双目标定所求的基础矩阵(含RT)，分解成左右相机各旋转平移一半为Rl,Tl与Rr,Tr，分解原则为左右图像重投影畸变最小，左右的共同面积最大。</p><p>旋转后，左右相机的光轴平行，两成像面平行，没有行对准。</p><p>构造旋转矩阵使得基线与成像平面平行，构造方法通过右相机相对左相机的偏移矩阵T完成。</p><p>为了使得左相机的极点变换到无穷远（极线水平），左右相机投影中心之间的平移向量就是左极线方向。</p><h3 id="2、计算"><a href="#2、计算" class="headerlink" title="2、计算"></a>2、计算</h3><p>假设空间中一点P，其在世界坐标系下的坐标为Pw，左相机到右相机的关系为R,T，其在左右相机坐标系下的坐标可以表示为：</p><p>$$<br>\begin{cases}<br>P_{l} &#x3D;  R_lP_w+T_l\<br>P_{r} &#x3D;  R_rP_w+T_r<br>\end{cases}<br>$$</p><p>其中</p><p>$$<br>P_r &#x3D; RP_l+T<br>$$</p><p>结合以上两个公式，推得：</p><p>$$<br>\begin{cases}<br>R &#x3D; R_rR_l^T\<br>T &#x3D; T_r-RT_l<br>\end{cases}<br>$$</p><p>奇异值分解？</p><h2 id="五、眼手标定"><a href="#五、眼手标定" class="headerlink" title="五、眼手标定"></a>五、眼手标定</h2><p>眼手标定根据相机的安装方式不同，分别求解的是相机到机械手末端轴的坐标变换关系(相机在手上)</p><p>$$<br>{^T}T{_C}<br>$$<br>或相机到机械手底座的坐标变换关系(相机不在手上)</p><p>$$<br>{^R}T{_C}<br>$$</p><h3 id="1、相机安装在机械手上"><a href="#1、相机安装在机械手上" class="headerlink" title="1、相机安装在机械手上"></a>1、相机安装在机械手上</h3><p>此种相机安装方式(Eye in hand)，<strong>标定量为相机与机械手末端之间的关系</strong>。</p><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/eye_in_hand.png"/></center><p>标定方法: 固定标定板与机械手底座不变，移动机械手至两个位置1、2，分别记录两个位置下的相机图像，机械手位置。</p><p>假设空间中一物体(object)，机械手需要使用(眼在手上的相机)拍摄物体，得到物体在图片中的位置Pimg(x,y)，通过之前的单目相机标定，能够将此坐标转换到相机坐标系下的表示，即：<br>$$<br>P_{cam} &#x3D; {^C}T{<em>O} * P</em>{img}<br>$$</p><p>然而，控制机械手抓取物体的动作，调整的是基于机械手底座(Robot)坐标系的xyzuvw变化，所以还需要求解相机坐标系到机械手底座坐标系的转换，即：</p><p>$$<br>{^R}T{_C} &#x3D; {^R}T{_T} {^T}T{_C}<br>$$</p><p>${^R}T{_T}$：机械手末端到机械手底座<br>${^T}T{_C}$：相机到机械手末端</p><p>在已知机械手末端到机械手底座关系的条件下，未知量只有相机到机械手末端转换矩阵：</p><p>$$<br>{^T}T{_C}<br>$$<br>移动机械手分别到位置1与位置2，<strong>以标定板到底座的固定关系(转换关系不变)，建立1、2两个位置下的恒等式。</strong></p><p>令：$X &#x3D; {^T}T{_C}$<br>$$<br>{^R}T{_O} &#x3D; {^R}T1{_T}\cdot X\cdot {^C}T1{_O} &#x3D; {^R}T2{_T}\cdot X\cdot {^C}T2{_O}<br>$$</p><p>以上方程式的未知数只有X一个，可以将方程化成：</p><p>$$<br>{^R}T2{^{-1}_T}\cdot{  {^R}T1{_T}\cdot X \cdot {^C}T1{_O} } \cdot {^C}T1{^{-1}_O} &#x3D;{^R}T2{^{-1}_T} \cdot{  {^R}T2{_T} \cdot X \cdot {^C}T2{_O} } \cdot {^C}T1{^{-1}_O}<br>$$</p><p>即：</p><!-- $${  {^R}T1{_T}\cdot X = X \cdot {^C}T2{_O}}$$ --><p>$$<br>{^R}T2{^{-1}_T}\cdot{  {^R}T1{_T}\cdot X &#x3D; X \cdot {^C}T2{_O} } \cdot {^C}T1{^{-1}_O}<br>$$</p><p>化简为：</p><p>$$<br>A \cdot X &#x3D; X \cdot B<br>$$</p><h3 id="2、相机不安装在机械手上"><a href="#2、相机不安装在机械手上" class="headerlink" title="2、相机不安装在机械手上"></a>2、相机不安装在机械手上</h3><p>此种相机安装方式(Eye to hand)需要将标定板固定在机械手末端，保持机械手末端(Tool)与物体(Object)的位置关系不变，<strong>标定量为相机到机械手底座之间的关系</strong>。</p><center><img src="https://raw.githubusercontent.com/JockerLin/NotesImageLib/master/eye_to_hand.png"/></center><p>方法同 <a href="###1%E7%9B%B8%E6%9C%BA%E5%AE%89%E8%A3%85%E5%9C%A8%E6%9C%BA%E6%A2%B0%E6%89%8B%E4%B8%8A">相机安装在机械手上</a>，分别移动机械手到位置1与位置2，<strong>以标定板到机械手末端的固定关系(转换关系不变)，建立1、2两个位置下的恒等式。</strong></p><p>令：$X &#x3D; {^R}T{_C}$<br>$$<br>{^T}T{_O} &#x3D; {^T}T1{_R}\cdot X\cdot {^C}T1{_O} &#x3D; {^T}T2{_R}\cdot X\cdot {^C}T2{_O}<br>$$</p><p>以上方程式的未知数只有X一个，可以将方程化成：</p><p>$$<br>{^T}T2{^{-1}_R}\cdot </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;相机标定大全——平面、单目、双目、眼手&quot;&gt;&lt;a href=&quot;#相机标定大全——平面、单目、双目、眼手&quot; class=&quot;headerlink&quot; title=&quot;相机标定大全——平面、单目、双目、眼手&quot;&gt;&lt;/a&gt;相机标定大全——平面、单目、双目、眼手&lt;/h1&gt;&lt;p&gt;看完</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Note Book</title>
    <link href="https://jockerlin.github.io/2023/05/27/%E6%95%B4%E5%90%88/"/>
    <id>https://jockerlin.github.io/2023/05/27/%E6%95%B4%E5%90%88/</id>
    <published>2023-05-27T10:18:08.330Z</published>
    <updated>2023-05-27T12:08:24.186Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Note-Book"><a href="#Note-Book" class="headerlink" title="Note Book"></a>Note Book</h1><p>逐步将平台转移至Notion～</p><p><a href="https://www.notion.so/blazarlin/Personal-Home-eb4a8d5b6c80428eba0b51b05120c271">Notion Home Page</a></p><h2 id="Personal-Summary"><a href="#Personal-Summary" class="headerlink" title="Personal Summary"></a>Personal Summary</h2><h3 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h3><ul><li><p><a href="family/calibration.md">Camera Calibration</a></p></li><li><p><a href="family/imgprocess.md">2D Imgprocess</a></p></li><li><p><a href="family/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E8%A6%81%E7%82%B9%E8%AE%B0%E5%BD%95.md">Digital Image Process</a></p></li><li><p><a href="family/barcode_detection.md">Barcode</a></p></li><li><p><a href="family/visual_servoing.md">Visual Servoing</a></p></li><li><p><a href="family/3dvision.md">3D Vision</a></p></li></ul><h3 id="Programer"><a href="#Programer" class="headerlink" title="Programer"></a>Programer</h3><ul><li><p><a href="family/c++_learning.md">c++</a></p></li><li><p><a href="family/python_skills.md">python</a></p></li></ul><h3 id="Algorithmic"><a href="#Algorithmic" class="headerlink" title="Algorithmic"></a>Algorithmic</h3><ul><li><p><a href="family/algorithm.md">alg</a></p></li><li><p><a href="family/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6.md">Algorithmic Complexity</a></p></li><li><p><a href="family/fittinggeometry.md">Fit circle&amp;ellipse</a></p></li></ul><h3 id="SoftWare-developer"><a href="#SoftWare-developer" class="headerlink" title="SoftWare developer"></a>SoftWare developer</h3><ul><li><a href="family/software_development.md">pyqt</a></li></ul><h3 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h3><ul><li><p><a href="family/ubuntu%E5%9F%BA%E6%93%8D.md">ubuntu基操</a></p></li><li><p><a href="family/git_skills.md">git</a></p></li><li><p><a href="family/docker.md">Docker</a></p></li></ul><h3 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h3><ul><li><p><a href="family/machine_learning.md">my notes</a></p></li><li><p><a href="family/faster_rcnn-exp.md">FasterRcnn实验记录</a></p></li><li><p><a href="family/work_record.md">work record</a></p></li></ul><h2 id="Appendix-amp-Help"><a href="#Appendix-amp-Help" class="headerlink" title="Appendix&amp;Help"></a>Appendix&amp;Help</h2><ul><li><p><a href="family/md%E6%96%B0%E6%89%8B.md">markdown基本语法</a></p></li><li><p><a href="family/mathjax_cmdeditor.md">Markdown 公式指导手册</a></p></li><li><p><a href="family/samble.md">数学符号</a></p></li><li><p><a href="family/pdf/LATEX_Mathematical_Symbols.pdf">Markdown 数学公式</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Note-Book&quot;&gt;&lt;a href=&quot;#Note-Book&quot; class=&quot;headerlink&quot; title=&quot;Note Book&quot;&gt;&lt;/a&gt;Note Book&lt;/h1&gt;&lt;p&gt;逐步将平台转移至Notion～&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://ww</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jockerlin.github.io/2023/05/27/hello-world/"/>
    <id>https://jockerlin.github.io/2023/05/27/hello-world/</id>
    <published>2023-05-27T08:39:39.897Z</published>
    <updated>2023-05-27T08:39:39.897Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
